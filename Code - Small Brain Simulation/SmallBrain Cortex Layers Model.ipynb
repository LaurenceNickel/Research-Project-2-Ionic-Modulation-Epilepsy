{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "27fc590d-d3fa-4c9c-9091-ce65833ce1ca",
   "metadata": {},
   "source": [
    "<h1 style=\"font-size: 50px;\">Research Project 2 Epilepsy Ionic Modulation - SmallBrain Cortex Layers</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "775d45db-ecde-4107-817b-22d1d7d4bbd5",
   "metadata": {},
   "source": [
    "This file contains the code for running a simulation where the model used is build by mimicking the layers present in the neocortex: L1, L2/L3, L4, L5, and L6. For each layer, the neuron densities and volumes were retrieved from literature and were used to generate ~17000 neurons across the entire 3D space."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dbc7291-82c6-44cd-bf20-c4b008e74319",
   "metadata": {},
   "source": [
    "<br></br><br></br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad949241-52a7-44c4-aa78-34ef69aac4a8",
   "metadata": {},
   "source": [
    "<h2 style=\"font-size: 40px;\">Installing and Importing Libraries</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4f62ff77-02e0-403a-92d7-862c41466727",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-19T09:40:58.003345Z",
     "start_time": "2024-06-19T09:35:28.326945Z"
    }
   },
   "outputs": [],
   "source": [
    "# Installing all the required libraries.\n",
    "!pip install -q jupyter\n",
    "!pip install -q matplotlib\n",
    "!pip install -q numpy\n",
    "!pip install -q pandas\n",
    "!pip install -q plotly\n",
    "!pip install -q scipy\n",
    "!pip install -q tqdm\n",
    "!pip install -q Brian2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fc1719df-92c4-40dc-955a-014a5520b762",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing all the required libraries.\n",
    "from plots import *\n",
    "from equations import *\n",
    "from global_settings import *\n",
    "from masks import *\n",
    "from helper import *\n",
    "from run_loop import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4571ab25-9c49-4338-b976-6d70d9f232e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from brian2 import *\n",
    "import random\n",
    "from itertools import chain, zip_longest\n",
    "from tqdm.notebook import tqdm\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import time\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50a64da5-9001-4f34-88e0-65ab7a52d6d5",
   "metadata": {},
   "source": [
    "<br></br><br></br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "400b8eac-14cb-41f7-9f52-43ec07831c66",
   "metadata": {},
   "source": [
    "<h2 style=\"font-size: 40px;\">General Functions</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e0cc288b-240a-44cd-b832-2981ae76846f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reads the stimulus input signal.\n",
    "def read_input_signal(file_name):\n",
    "    in_1 = np.loadtxt('./stimuli/'+file_name)\n",
    "\n",
    "    # Converting the signal to be of type 'TimedArray' with a specified time step which can be used in the simulation.\n",
    "    input_signal = TimedArray(in_1*namp,dt=defaultclock.dt)\n",
    "\n",
    "    return input_signal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc359126-3c97-46d7-b487-0f9038849406",
   "metadata": {},
   "source": [
    "<br></br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e5655bd2-c00e-4dcd-9d6e-4a624facd0df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking whether the mask with the provided coordinate center and radius can be applied to the list of layers.\n",
    "def checking_mask(shape_mask, list_of_layers, center_coordinates, radius_or_edge_length, layer_names, layer_geometrics):\n",
    "\n",
    "    # Checking if the 'list_of_layers' is empty.\n",
    "    if len(list_of_layers) == 0:\n",
    "        raise ValueError(\"The 'list_of_layers' list is empty.\")\n",
    "    \n",
    "    # Checking if the 'list_of_layers' only contains valid layer names.\n",
    "    if not all(layer in layer_names for layer in list_of_layers):\n",
    "        raise ValueError(\"The 'list_of_layers' list contains an invalid layer name.\")\n",
    "    \n",
    "    # Checking if there are no duplicate layer in the 'list_of_layers'.\n",
    "    if len(list_of_layers) != len(set(list_of_layers)):\n",
    "        raise ValueError(\"The 'list_of_layers' list contains duplicate layers.\")\n",
    "\n",
    "    # Checking that the 'list_of_layers' list only contains layers that are adjacent to each other (otherwise the stimulus mask would pass through layers that are not in this list).\n",
    "    if len(list_of_layers) > 1:\n",
    "        layer_indices = [layer_names.index(layer) for layer in list_of_layers]\n",
    "        if not all(layer_indices[i] + 1 == layer_indices[i+1] for i in range(len(layer_indices) - 1)):\n",
    "            raise ValueError(\"The 'list_of_layers' list contains layers that are not adjacent to each other.\")\n",
    "\n",
    "    # Checking if the coordinates of the center of the mask are positive.\n",
    "    if not (center_coordinates[0] > 0 and center_coordinates[1] > 0 and center_coordinates[2] > 0):\n",
    "        raise ValueError(\"The coordinates of the center of the mask should be positive.\")\n",
    "\n",
    "    # Checking if the radius of the mask is positive.\n",
    "    if not (radius_or_edge_length > 0):\n",
    "        raise ValueError(\"The radius of the mask should be positive.\")\n",
    "\n",
    "    # Generating 'num_edge_points' random points that are located on the edge of the mask which is either spherical, cubical, or all.\n",
    "    num_edge_points = 10000\n",
    "    if shape_mask == \"spherical\":\n",
    "        radius = radius_or_edge_length\n",
    "        edge_points = generating_n_random_points_spherical_edge(num_edge_points, center_coordinates, radius)\n",
    "        \n",
    "    elif shape_mask == \"cubical\":\n",
    "        edge_length = radius_or_edge_length\n",
    "        edge_points = generating_n_random_points_cubical_edge(num_edge_points, center_coordinates, edge_length)\n",
    "        \n",
    "    else:\n",
    "        raise ValueError(\"The shape of the mask given is not valid. Please select either 'spherical' or 'cubical'.\")\n",
    "\n",
    "    # Checking whether every point present in the 'edge_points' is located within the list of layers.\n",
    "    for edge_point in edge_points:\n",
    "        point_in_layer = False\n",
    "        for layer in list_of_layers:\n",
    "            match layer:\n",
    "                case \"L1\":\n",
    "                    point_in_layer = check_point_within_layer(edge_point, 'L1', layer_geometrics)\n",
    "                case \"L2_L3\":\n",
    "                    point_in_layer = check_point_within_layer(edge_point, 'L2_L3', layer_geometrics)\n",
    "                case \"L4\":\n",
    "                    point_in_layer = check_point_within_layer(edge_point, 'L4', layer_geometrics)\n",
    "                case \"L5\":\n",
    "                    point_in_layer = check_point_within_layer(edge_point, 'L5', layer_geometrics)\n",
    "                case \"L6\":\n",
    "                    point_in_layer = check_point_within_layer(edge_point, 'L6', layer_geometrics)\n",
    "            if point_in_layer:\n",
    "                break\n",
    "        if not point_in_layer:\n",
    "            raise ValueError(\"Not every point present in the mask will be within the defined layers.\")\n",
    "\n",
    "    return [center_coordinates, radius_or_edge_length]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed147088-bcdb-47c8-aaba-0ffa33729c5d",
   "metadata": {},
   "source": [
    "<br></br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5d45825d-afcf-4577-a569-b6cfa46c59d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating n random points that are located on the edge of the spherical mask.\n",
    "def generating_n_random_points_spherical_edge(num_points, center_coordinates, radius):\n",
    "\n",
    "    # Looping 'num_points' times and for each loop generating a random point on the edge of the spherical mask.\n",
    "    points = []\n",
    "    for i in range(num_points):\n",
    "        # Generating the components needed to generate a point on the edge exactly 'radius' away from the center coordinates.\n",
    "        phi = np.random.uniform(0, 2 * np.pi)\n",
    "        costheta = np.random.uniform(-1, 1)\n",
    "        theta = np.arccos(costheta)\n",
    "\n",
    "        # Generating a random point that is located on the edge of the spherical mask.\n",
    "        x = center_coordinates[0] + radius * np.sin(theta) * np.cos(phi)\n",
    "        y = center_coordinates[1] + radius * np.sin(theta) * np.sin(phi)\n",
    "        z = center_coordinates[2] + radius * np.cos(theta)\n",
    "\n",
    "        # Appending the randomly generated point to the 'points' array.\n",
    "        points.append([x, y, z])\n",
    "        \n",
    "    return points"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48c9cf37-8a9d-47fb-aba2-0e6ea3a09183",
   "metadata": {},
   "source": [
    "<br></br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "38edcf89-9b3b-4d04-80d1-a385d112fc62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating n random points that are located on the edge of the cubical mask.\n",
    "def generating_n_random_points_cubical_edge(num_points, center_coordinates, edge_length):\n",
    "\n",
    "    # Looping 'num_points' times and for each loop generating a random point on the edge of the cubical mask.\n",
    "    points = []\n",
    "    for i in range(num_points):\n",
    "        \n",
    "        # Generating a random face of the cube on which the random point will be located.\n",
    "        face = np.random.randint(0, 6)  \n",
    "\n",
    "        # Depending on the face generated, a random point will be created.\n",
    "        match face:\n",
    "        \n",
    "            # The right hand side face.\n",
    "            case 0: \n",
    "                x = center_coordinates[0] - edge_length/2\n",
    "                y = np.random.uniform(center_coordinates[1] - edge_length/2, center_coordinates[1] + edge_length/2)\n",
    "                z = np.random.uniform(center_coordinates[2] - edge_length/2, center_coordinates[2] + edge_length/2)\n",
    "\n",
    "            # The left hand side face.\n",
    "            case 1:\n",
    "                x = center_coordinates[0] + edge_length/2\n",
    "                y = np.random.uniform(center_coordinates[1] - edge_length/2, center_coordinates[1] + edge_length/2)\n",
    "                z = np.random.uniform(center_coordinates[2] - edge_length/2, center_coordinates[2] + edge_length/2)\n",
    "\n",
    "            # The bottom face.\n",
    "            case 2:\n",
    "                x = np.random.uniform(center_coordinates[0] - edge_length/2, center_coordinates[0] + edge_length/2)\n",
    "                y = center_coordinates[1] - edge_length/2\n",
    "                z = np.random.uniform(center_coordinates[2] - edge_length/2, center_coordinates[2] + edge_length/2)\n",
    "\n",
    "            # The top face.\n",
    "            case 3:\n",
    "                x = np.random.uniform(center_coordinates[0] - edge_length/2, center_coordinates[0] + edge_length/2)\n",
    "                y = center_coordinates[1] + edge_length/2\n",
    "                z = np.random.uniform(center_coordinates[2] - edge_length/2, center_coordinates[2] + edge_length/2)\n",
    "\n",
    "            # The front face.\n",
    "            case 4:\n",
    "                x = np.random.uniform(center_coordinates[0] - edge_length/2, center_coordinates[0] + edge_length/2)\n",
    "                y = np.random.uniform(center_coordinates[1] - edge_length/2, center_coordinates[1] + edge_length/2)\n",
    "                z = center_coordinates[2] - edge_length/2\n",
    "\n",
    "            # The rear face.\n",
    "            case 5:\n",
    "                x = np.random.uniform(center_coordinates[0] - edge_length/2, center_coordinates[0] + edge_length/2)\n",
    "                y = np.random.uniform(center_coordinates[1] - edge_length/2, center_coordinates[1] + edge_length/2)\n",
    "                z = center_coordinates[2] + edge_length/2\n",
    "\n",
    "        # Appending the randomly generated point to the 'points' array.\n",
    "        points.append([x, y, z])\n",
    "\n",
    "    return points"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1fe1ca8-0182-4a6a-bec1-b6ffbbeff886",
   "metadata": {},
   "source": [
    "<br></br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "39940278-ed1c-44cf-93f6-60022745ffc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking whether the point is located within the current layer.\n",
    "def check_point_within_layer(point, layer, layer_geometrics):\n",
    "\n",
    "    # Retrieving the x-coordinate, y-coordinate, and z-coordinate from the 'point' parameter.\n",
    "    x_point, y_point, z_point = point\n",
    "\n",
    "    # Determining for which layer the point should be checked.\n",
    "    match layer:\n",
    "        case \"L1\":\n",
    "            layer_block = layer_geometrics['L1_block']\n",
    "        case \"L2_L3\":\n",
    "            layer_block = layer_geometrics['L2_L3_block']\n",
    "        case \"L4\":\n",
    "            layer_block = layer_geometrics['L4_block']\n",
    "        case \"L5\":\n",
    "            layer_block = layer_geometrics['L5_block']\n",
    "        case \"L6\":\n",
    "            layer_block = layer_geometrics['L6_block']\n",
    "\n",
    "    # Retrieving the bottom left x-coordinate, y-coordinate, and z-coordinate of the current layer from the 'layer_geometrics' parameter.\n",
    "    layer_bottom_left_x, layer_bottom_left_y, layer_bottom_left_z  = layer_block['bottom_left_position']\n",
    "\n",
    "    # Retrieving the width, height, and width of the current layer from the 'layer_geometrics' parameter.\n",
    "    layer_width = layer_block['width']\n",
    "    layer_height = layer_block['height']\n",
    "    layer_depth = layer_block['depth']\n",
    "\n",
    "    # Checking whether the point is located within the current layer.\n",
    "    if ((x_point >= layer_bottom_left_x and x_point <= layer_bottom_left_x + layer_width) and\n",
    "        (y_point >= layer_bottom_left_y and y_point <= layer_bottom_left_y + layer_height) and\n",
    "        (z_point >= layer_bottom_left_z and z_point <= layer_bottom_left_z + layer_depth)):\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "808505a2-0078-441a-93d5-5d33446fd7a4",
   "metadata": {},
   "source": [
    "<br></br><br></br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09a47486-78e3-40d8-a25a-6336560a84dd",
   "metadata": {},
   "source": [
    "<h2 style=\"font-size: 40px;\">Topology and Neuron Groups Functions</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ef688cb0-19e8-433a-aef0-b918c5b9350c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the complete neuron topology of the model taking into account the structure of the cortex.\n",
    "def create_complete_neuron_topology_cortex(desired_total_num_of_neurons, layer_neuron_densities, layer_volumes, layer_excitatory_ratios, layer_names):\n",
    "\n",
    "    ###########################################################\n",
    "    ########  Calculating Number of Neurons per layer  ########\n",
    "    ###########################################################\n",
    "    # Initializing a dictionary that will contain the number of neurons present in each of the layers and a variable that counts the total numbers of neurons.\n",
    "    num_of_neurons = {}\n",
    "    total_num_of_neurons = 0\n",
    "\n",
    "    # Looping over every layer and calculating how many neurons are present in each of the layers depending on the neuron densities and the volumes of the layers.\n",
    "    for layer in layer_names:\n",
    "        num_of_neurons[layer] = layer_neuron_densities[layer] * layer_volumes[layer]\n",
    "        total_num_of_neurons += num_of_neurons[layer]\n",
    "\n",
    "    # Calculating the scaling factor for the number of neurons for all layers as we ideally only want to model a number similar to the 'desired_total_num_of_neurons'.\n",
    "    neuron_scaling_factor = desired_total_num_of_neurons / total_num_of_neurons\n",
    "\n",
    "    # Initializing a dictionary that will contain the adjusted number of neurons present in each of the layers and one that will contain the adjusted volume of each layer.\n",
    "    num_of_neurons_adjusted = {}\n",
    "    layer_volume_adjusted = {}\n",
    "\n",
    "    # Looping over every layer and calculating how many neurons are present in each of the layers after having multiplied the original number with the 'neuron_scaling_factor'.\n",
    "    for layer in layer_names:\n",
    "        num_of_neurons_adjusted[layer] = neuron_scaling_factor * num_of_neurons[layer]\n",
    "        layer_volume_adjusted[layer] = neuron_scaling_factor * layer_volumes[layer]\n",
    "\n",
    "    # Creating a dictionary that will eventually contain all the layer information.\n",
    "    layer_data = {}\n",
    "    for layer in layer_names:\n",
    "        layer_data[layer] = {\"layer_name\": layer,\n",
    "                             \"volume\": layer_volume_adjusted[layer],\n",
    "                             \"num_excitatory_neurons\": int(layer_excitatory_ratios[layer] * num_of_neurons_adjusted[layer]),\n",
    "                             \"num_inhibitory_neurons\": int((1 - layer_excitatory_ratios[layer]) * num_of_neurons_adjusted[layer])}\n",
    "\n",
    "    # Initializing a dictionary that will eventually contain the geometrical information of the different building blocks of the layers.\n",
    "    layer_geometrics = {}\n",
    "\n",
    "    \n",
    "    ##############################################################\n",
    "    ########  Calculating Width and Depth of Every Layer  ########\n",
    "    ##############################################################\n",
    "    # Calculating the width and depth of every layer (which is the same for every layer) by focusing on 1 layer and defining its width:height:depth ratio. For this we will use layer L1 and and use a ratio of 1:0.5:1.\n",
    "    width_layers = (2 * layer_data['L1']['volume']) ** (1/3)\n",
    "    depth_layers = width_layers\n",
    "\n",
    "    \n",
    "    #####################################################\n",
    "    ########  Calculating Height of Every Layer  ########\n",
    "    #####################################################\n",
    "    # Calculating the height of every layer by dividing the volume of each layer by the product of the calculated constant width and depth.\n",
    "    L1_height = layer_data['L1']['volume'] / (width_layers * depth_layers)\n",
    "    L2_L3_height = layer_data['L2_L3']['volume'] / (width_layers * depth_layers)\n",
    "    L4_height = layer_data['L4']['volume'] / (width_layers * depth_layers)\n",
    "    L5_height = layer_data['L5']['volume'] / (width_layers * depth_layers)\n",
    "    L6_height = layer_data['L6']['volume'] / (width_layers * depth_layers)\n",
    "\n",
    "\n",
    "    #####################################################\n",
    "    ########  Geometric Information of Layer L6  ########\n",
    "    #####################################################\n",
    "    # Adding the geometrical information of layer L6 to the 'layer_geometrics' database.\n",
    "    L6_x = 0\n",
    "    L6_y = 0\n",
    "    L6_z = 0\n",
    "    layer_geometrics['L6_block'] = {\"width\": width_layers,\n",
    "                                    \"height\": L6_height,\n",
    "                                    \"depth\": depth_layers,\n",
    "                                    \"bottom_left_position\": [L6_x, L6_y, L6_z],\n",
    "                                    \"num_excitatory_neurons\": layer_data['L1']['num_excitatory_neurons'],\n",
    "                                    \"num_inhibitory_neurons\": layer_data['L1']['num_inhibitory_neurons']\n",
    "                                    }\n",
    "\n",
    "\n",
    "    #####################################################\n",
    "    ########  Geometric Information of Layer L5  ########\n",
    "    #####################################################\n",
    "    # Adding the geometrical information of layer L5 to the 'layer_geometrics' database.\n",
    "    L5_x = 0\n",
    "    L5_y = L6_height\n",
    "    L5_z = 0\n",
    "    layer_geometrics['L5_block'] = {\"width\": width_layers,\n",
    "                                    \"height\": L5_height,\n",
    "                                    \"depth\": depth_layers,\n",
    "                                    \"bottom_left_position\": [L5_x, L5_y, L5_z],\n",
    "                                    \"num_excitatory_neurons\": layer_data['L1']['num_excitatory_neurons'],\n",
    "                                    \"num_inhibitory_neurons\": layer_data['L1']['num_inhibitory_neurons']\n",
    "                                    }\n",
    "\n",
    "\n",
    "    #####################################################\n",
    "    ########  Geometric Information of Layer L4  ########\n",
    "    #####################################################\n",
    "    # Adding the geometrical information of layer L4 to the 'layer_geometrics' database.\n",
    "    L4_x = 0\n",
    "    L4_y = L5_y + L5_height\n",
    "    L4_z = 0\n",
    "    layer_geometrics['L4_block'] = {\"width\": width_layers,\n",
    "                                    \"height\": L4_height,\n",
    "                                    \"depth\": depth_layers,\n",
    "                                    \"bottom_left_position\": [L4_x, L4_y, L4_z],\n",
    "                                    \"num_excitatory_neurons\": layer_data['L1']['num_excitatory_neurons'],\n",
    "                                    \"num_inhibitory_neurons\": layer_data['L1']['num_inhibitory_neurons']\n",
    "                                    }\n",
    "    \n",
    "    \n",
    "    ########################################################\n",
    "    ########  Geometric Information of Layer L2_L3  ########\n",
    "    ########################################################\n",
    "    # Adding the geometrical information of layer L2_L3 to the 'layer_geometrics' database.\n",
    "    L2_L3_x = 0\n",
    "    L2_L3_y = L4_y + L4_height\n",
    "    L2_L3_z = 0\n",
    "    layer_geometrics['L2_L3_block'] = {\"width\": width_layers,\n",
    "                                    \"height\": L2_L3_height,\n",
    "                                    \"depth\": depth_layers,\n",
    "                                    \"bottom_left_position\": [L2_L3_x, L2_L3_y, L2_L3_z],\n",
    "                                    \"num_excitatory_neurons\": layer_data['L1']['num_excitatory_neurons'],\n",
    "                                    \"num_inhibitory_neurons\": layer_data['L1']['num_inhibitory_neurons']\n",
    "                                    }\n",
    "\n",
    "    \n",
    "    #####################################################\n",
    "    ########  Geometric Information of Layer L1  ########\n",
    "    #####################################################\n",
    "    # Adding the geometrical information of layer L1 to the 'layer_geometrics' database.\n",
    "    L1_x = 0\n",
    "    L1_y = L2_L3_y + L2_L3_height\n",
    "    L1_z = 0\n",
    "    layer_geometrics['L1_block'] = {\"width\": width_layers,\n",
    "                                    \"height\": L1_height,\n",
    "                                    \"depth\": depth_layers,\n",
    "                                    \"bottom_left_position\": [L1_x, L1_y, L1_z],\n",
    "                                    \"num_excitatory_neurons\": layer_data['L1']['num_excitatory_neurons'],\n",
    "                                    \"num_inhibitory_neurons\": layer_data['L1']['num_inhibitory_neurons']\n",
    "                                    }\n",
    "    \n",
    "    \n",
    "    ############################################################\n",
    "    ########  Generating Random Points for Every Layer  ########\n",
    "    ############################################################\n",
    "    # Initializing two dictionaries that will for each layer store the positions of the excitatory and inhibitory neurons.\n",
    "    excitatory_positions = {}\n",
    "    inhibitory_positions = {}\n",
    "    \n",
    "    ########\n",
    "    ## L1 ##\n",
    "    ########   \n",
    "    # Generating a set of random x, y, and z positions that fall within layer L1 for the excitatory neurons.\n",
    "    L1_x_exc = [random.uniform(layer_geometrics['L1_block']['bottom_left_position'][0], layer_geometrics['L1_block']['bottom_left_position'][0] + layer_geometrics['L1_block']['width']) for _ in range(layer_geometrics['L1_block']['num_excitatory_neurons'])]\n",
    "    L1_y_exc = [random.uniform(layer_geometrics['L1_block']['bottom_left_position'][1], layer_geometrics['L1_block']['bottom_left_position'][1] + layer_geometrics['L1_block']['height']) for _ in range(layer_geometrics['L1_block']['num_excitatory_neurons'])]\n",
    "    L1_z_exc = [random.uniform(layer_geometrics['L1_block']['bottom_left_position'][2], layer_geometrics['L1_block']['bottom_left_position'][2] + layer_geometrics['L1_block']['depth']) for _ in range(layer_geometrics['L1_block']['num_excitatory_neurons'])]\n",
    "    L1_excitatory_topology = np.array([L1_x_exc, L1_y_exc, L1_z_exc])\n",
    "\n",
    "    # Generating a set of random x, y, and z positions that fall within layer L1 for the inhibitory neurons.\n",
    "    L1_x_inh = [random.uniform(layer_geometrics['L1_block']['bottom_left_position'][0], layer_geometrics['L1_block']['bottom_left_position'][0] + layer_geometrics['L1_block']['width']) for _ in range(layer_geometrics['L1_block']['num_inhibitory_neurons'])]\n",
    "    L1_y_inh = [random.uniform(layer_geometrics['L1_block']['bottom_left_position'][1], layer_geometrics['L1_block']['bottom_left_position'][1] + layer_geometrics['L1_block']['height']) for _ in range(layer_geometrics['L1_block']['num_inhibitory_neurons'])]\n",
    "    L1_z_inh = [random.uniform(layer_geometrics['L1_block']['bottom_left_position'][2], layer_geometrics['L1_block']['bottom_left_position'][2] + layer_geometrics['L1_block']['depth']) for _ in range(layer_geometrics['L1_block']['num_inhibitory_neurons'])]\n",
    "    L1_inhibitory_topology = np.array([L1_x_inh, L1_y_inh, L1_z_inh])\n",
    "\n",
    "    # Adding the excitatory and inhibitory topologies of the L1 layer to the 'excitatory_positions' and 'inhibitory_positions' dictionaries.\n",
    "    excitatory_positions['L1'] = L1_excitatory_topology\n",
    "    inhibitory_positions['L1'] = L1_inhibitory_topology\n",
    "\n",
    "    \n",
    "    ########\n",
    "    ## L2_L3 ##\n",
    "    ########   \n",
    "    # Generating a set of random x, y, and z positions that fall within layer L2_L3 for the excitatory neurons.\n",
    "    L2_L3_x_exc = [random.uniform(layer_geometrics['L2_L3_block']['bottom_left_position'][0], layer_geometrics['L2_L3_block']['bottom_left_position'][0] + layer_geometrics['L2_L3_block']['width']) for _ in range(layer_geometrics['L2_L3_block']['num_excitatory_neurons'])]\n",
    "    L2_L3_y_exc = [random.uniform(layer_geometrics['L2_L3_block']['bottom_left_position'][1], layer_geometrics['L2_L3_block']['bottom_left_position'][1] + layer_geometrics['L2_L3_block']['height']) for _ in range(layer_geometrics['L2_L3_block']['num_excitatory_neurons'])]\n",
    "    L2_L3_z_exc = [random.uniform(layer_geometrics['L2_L3_block']['bottom_left_position'][2], layer_geometrics['L2_L3_block']['bottom_left_position'][2] + layer_geometrics['L2_L3_block']['depth']) for _ in range(layer_geometrics['L2_L3_block']['num_excitatory_neurons'])]\n",
    "    L2_L3_excitatory_topology = np.array([L2_L3_x_exc, L2_L3_y_exc, L2_L3_z_exc])\n",
    "\n",
    "    # Generating a set of random x, y, and z positions that fall within layer L2_L3 for the inhibitory neurons.\n",
    "    L2_L3_x_inh = [random.uniform(layer_geometrics['L2_L3_block']['bottom_left_position'][0], layer_geometrics['L2_L3_block']['bottom_left_position'][0] + layer_geometrics['L2_L3_block']['width']) for _ in range(layer_geometrics['L2_L3_block']['num_inhibitory_neurons'])]\n",
    "    L2_L3_y_inh = [random.uniform(layer_geometrics['L2_L3_block']['bottom_left_position'][1], layer_geometrics['L2_L3_block']['bottom_left_position'][1] + layer_geometrics['L2_L3_block']['height']) for _ in range(layer_geometrics['L2_L3_block']['num_inhibitory_neurons'])]\n",
    "    L2_L3_z_inh = [random.uniform(layer_geometrics['L2_L3_block']['bottom_left_position'][2], layer_geometrics['L2_L3_block']['bottom_left_position'][2] + layer_geometrics['L2_L3_block']['depth']) for _ in range(layer_geometrics['L2_L3_block']['num_inhibitory_neurons'])]\n",
    "    L2_L3_inhibitory_topology = np.array([L2_L3_x_inh, L2_L3_y_inh, L2_L3_z_inh])\n",
    "\n",
    "    # Adding the excitatory and inhibitory topologies of the L2_L3 layer to the 'excitatory_positions' and 'inhibitory_positions' dictionaries.\n",
    "    excitatory_positions['L2_L3'] = L2_L3_excitatory_topology\n",
    "    inhibitory_positions['L2_L3'] = L2_L3_inhibitory_topology\n",
    "\n",
    "        \n",
    "    ########\n",
    "    ## L4 ##\n",
    "    ########   \n",
    "    # Generating a set of random x, y, and z positions that fall within layer L4 for the excitatory neurons.\n",
    "    L4_x_exc = [random.uniform(layer_geometrics['L4_block']['bottom_left_position'][0], layer_geometrics['L4_block']['bottom_left_position'][0] + layer_geometrics['L4_block']['width']) for _ in range(layer_geometrics['L4_block']['num_excitatory_neurons'])]\n",
    "    L4_y_exc = [random.uniform(layer_geometrics['L4_block']['bottom_left_position'][1], layer_geometrics['L4_block']['bottom_left_position'][1] + layer_geometrics['L4_block']['height']) for _ in range(layer_geometrics['L4_block']['num_excitatory_neurons'])]\n",
    "    L4_z_exc = [random.uniform(layer_geometrics['L4_block']['bottom_left_position'][2], layer_geometrics['L4_block']['bottom_left_position'][2] + layer_geometrics['L4_block']['depth']) for _ in range(layer_geometrics['L4_block']['num_excitatory_neurons'])]\n",
    "    L4_excitatory_topology = np.array([L4_x_exc, L4_y_exc, L4_z_exc])\n",
    "\n",
    "    # Generating a set of random x, y, and z positions that fall within layer L4 for the inhibitory neurons.\n",
    "    L4_x_inh = [random.uniform(layer_geometrics['L4_block']['bottom_left_position'][0], layer_geometrics['L4_block']['bottom_left_position'][0] + layer_geometrics['L4_block']['width']) for _ in range(layer_geometrics['L4_block']['num_inhibitory_neurons'])]\n",
    "    L4_y_inh = [random.uniform(layer_geometrics['L4_block']['bottom_left_position'][1], layer_geometrics['L4_block']['bottom_left_position'][1] + layer_geometrics['L4_block']['height']) for _ in range(layer_geometrics['L4_block']['num_inhibitory_neurons'])]\n",
    "    L4_z_inh = [random.uniform(layer_geometrics['L4_block']['bottom_left_position'][2], layer_geometrics['L4_block']['bottom_left_position'][2] + layer_geometrics['L4_block']['depth']) for _ in range(layer_geometrics['L4_block']['num_inhibitory_neurons'])]\n",
    "    L4_inhibitory_topology = np.array([L4_x_inh, L4_y_inh, L4_z_inh])\n",
    "\n",
    "    # Adding the excitatory and inhibitory topologies of the L4 layer to the 'excitatory_positions' and 'inhibitory_positions' dictionaries.\n",
    "    excitatory_positions['L4'] = L4_excitatory_topology\n",
    "    inhibitory_positions['L4'] = L4_inhibitory_topology\n",
    "\n",
    "    \n",
    "    ########\n",
    "    ## L5 ##\n",
    "    ########   \n",
    "    # Generating a set of random x, y, and z positions that fall within layer L5 for the excitatory neurons.\n",
    "    L5_x_exc = [random.uniform(layer_geometrics['L5_block']['bottom_left_position'][0], layer_geometrics['L5_block']['bottom_left_position'][0] + layer_geometrics['L5_block']['width']) for _ in range(layer_geometrics['L5_block']['num_excitatory_neurons'])]\n",
    "    L5_y_exc = [random.uniform(layer_geometrics['L5_block']['bottom_left_position'][1], layer_geometrics['L5_block']['bottom_left_position'][1] + layer_geometrics['L5_block']['height']) for _ in range(layer_geometrics['L5_block']['num_excitatory_neurons'])]\n",
    "    L5_z_exc = [random.uniform(layer_geometrics['L5_block']['bottom_left_position'][2], layer_geometrics['L5_block']['bottom_left_position'][2] + layer_geometrics['L5_block']['depth']) for _ in range(layer_geometrics['L5_block']['num_excitatory_neurons'])]\n",
    "    L5_excitatory_topology = np.array([L5_x_exc, L5_y_exc, L5_z_exc])\n",
    "\n",
    "    # Generating a set of random x, y, and z positions that fall within layer L5 for the inhibitory neurons.\n",
    "    L5_x_inh = [random.uniform(layer_geometrics['L5_block']['bottom_left_position'][0], layer_geometrics['L5_block']['bottom_left_position'][0] + layer_geometrics['L5_block']['width']) for _ in range(layer_geometrics['L5_block']['num_inhibitory_neurons'])]\n",
    "    L5_y_inh = [random.uniform(layer_geometrics['L5_block']['bottom_left_position'][1], layer_geometrics['L5_block']['bottom_left_position'][1] + layer_geometrics['L5_block']['height']) for _ in range(layer_geometrics['L5_block']['num_inhibitory_neurons'])]\n",
    "    L5_z_inh = [random.uniform(layer_geometrics['L5_block']['bottom_left_position'][2], layer_geometrics['L5_block']['bottom_left_position'][2] + layer_geometrics['L5_block']['depth']) for _ in range(layer_geometrics['L5_block']['num_inhibitory_neurons'])]\n",
    "    L5_inhibitory_topology = np.array([L5_x_inh, L5_y_inh, L5_z_inh])\n",
    "\n",
    "    # Adding the excitatory and inhibitory topologies of the L5 layer to the 'excitatory_positions' and 'inhibitory_positions' dictionaries.\n",
    "    excitatory_positions['L5'] = L5_excitatory_topology\n",
    "    inhibitory_positions['L5'] = L5_inhibitory_topology\n",
    "\n",
    "    \n",
    "    ########\n",
    "    ## L6 ##\n",
    "    ########   \n",
    "    # Generating a set of random x, y, and z positions that fall within layer L6 for the excitatory neurons.\n",
    "    L6_x_exc = [random.uniform(layer_geometrics['L6_block']['bottom_left_position'][0], layer_geometrics['L6_block']['bottom_left_position'][0] + layer_geometrics['L6_block']['width']) for _ in range(layer_geometrics['L6_block']['num_excitatory_neurons'])]\n",
    "    L6_y_exc = [random.uniform(layer_geometrics['L6_block']['bottom_left_position'][1], layer_geometrics['L6_block']['bottom_left_position'][1] + layer_geometrics['L6_block']['height']) for _ in range(layer_geometrics['L6_block']['num_excitatory_neurons'])]\n",
    "    L6_z_exc = [random.uniform(layer_geometrics['L6_block']['bottom_left_position'][2], layer_geometrics['L6_block']['bottom_left_position'][2] + layer_geometrics['L6_block']['depth']) for _ in range(layer_geometrics['L6_block']['num_excitatory_neurons'])]\n",
    "    L6_excitatory_topology = np.array([L6_x_exc, L6_y_exc, L6_z_exc])\n",
    "\n",
    "    # Generating a set of random x, y, and z positions that fall within layer L6 for the inhibitory neurons.\n",
    "    L6_x_inh = [random.uniform(layer_geometrics['L6_block']['bottom_left_position'][0], layer_geometrics['L6_block']['bottom_left_position'][0] + layer_geometrics['L6_block']['width']) for _ in range(layer_geometrics['L6_block']['num_inhibitory_neurons'])]\n",
    "    L6_y_inh = [random.uniform(layer_geometrics['L6_block']['bottom_left_position'][1], layer_geometrics['L6_block']['bottom_left_position'][1] + layer_geometrics['L6_block']['height']) for _ in range(layer_geometrics['L6_block']['num_inhibitory_neurons'])]\n",
    "    L6_z_inh = [random.uniform(layer_geometrics['L6_block']['bottom_left_position'][2], layer_geometrics['L6_block']['bottom_left_position'][2] + layer_geometrics['L6_block']['depth']) for _ in range(layer_geometrics['L6_block']['num_inhibitory_neurons'])]\n",
    "    L6_inhibitory_topology = np.array([L6_x_inh, L6_y_inh, L6_z_inh])\n",
    "\n",
    "    # Adding the excitatory and inhibitory topologies of the L6 layer to the 'excitatory_positions' and 'inhibitory_positions' dictionaries.\n",
    "    excitatory_positions['L6'] = L6_excitatory_topology\n",
    "    inhibitory_positions['L6'] = L6_inhibitory_topology\n",
    "\n",
    "    print(layer_geometrics)\n",
    "\n",
    "    \n",
    "    return layer_data, layer_geometrics, excitatory_positions, inhibitory_positions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5386177-571d-4242-90bc-97665e4d72a5",
   "metadata": {},
   "source": [
    "<br></br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a9391df9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates the group of excitatory neurons.\n",
    "def create_group_py(topology, noise, masks, current_layer, stimulus_layers, treatment_layers, excitatory_topologies_lengths, group_name='exc_group', integ_method='exponential_euler'):\n",
    "\n",
    "    # Extracting the passed on parameters.\n",
    "    x, y, z = topology\n",
    "    mu_noise, sigma_noise = noise\n",
    "    stimulus_mask, treatment_mask = masks\n",
    "\n",
    "    # Adjusting the group name.\n",
    "    group_name = group_name + \"_\" + current_layer\n",
    "\n",
    "    # Initializing a group of excitatory neurons with the following parameters:\n",
    "    # - py_eqs => Differential equations that define the behavior of the excitatory neuron.\n",
    "    # - threshold => Neurons fire an action potential when their membrane potential 'v' exceeds the threshold 'V_th'.\n",
    "    # - reset => Resets neuron states after they fire according to 'reset_eqs'.\n",
    "    # - refractory => Sets a refractory period during which an excitatory neuron cannot fire again.\n",
    "    # - method => Specifies the integration method for solving the differential equations.\n",
    "    G_exc = NeuronGroup(len(x),py_eqs,threshold='v>V_th',reset=reset_eqs,refractory=3*ms,name=group_name, method=integ_method)\n",
    "\n",
    "    # Initializing the membrane potential 'v' to be a random value between -60 mV and -100 mV.\n",
    "    G_exc.v = '-60*mvolt-rand()*40*mvolt'\n",
    "\n",
    "    # Sets the neurotransmitter to be used to be glutamate (which is an excitatory neurotransmitter).\n",
    "    G_exc.glu = 1\n",
    "\n",
    "    # Assigns the positions of the neurons in micrometers.\n",
    "    G_exc.x = x * um\n",
    "    G_exc.y = y * um\n",
    "    G_exc.z = z * um\n",
    "\n",
    "    # Sets the size of the neurons to 'taille_exc_normale'.\n",
    "    G_exc.taille = taille_exc_normale\n",
    "\n",
    "    # Sets the mean and standard deviation of the noise affecting the neurons.\n",
    "    G_exc.mu_noise = mu_noise\n",
    "    G_exc.sigma_noise = sigma_noise\n",
    "\n",
    "    ### FOR THE MASKS: Optionally allow for multiple layers to be part of the treatment/stimulus mask.\n",
    "    # Applying the treatment mask to the neuron by first checking whether the current layer is in the list of treatment layers. If not, a list of zeros is added as treatment mask.\n",
    "    if current_layer in treatment_layers:\n",
    "        G_exc.treatment_mask = treatment_mask\n",
    "    else:\n",
    "        G_exc.treatment_mask = np.zeros(len(x))\n",
    "\n",
    "    # Applying the stimulus mask to the neuron by first checking whether the current layer is in the list of stimulus layers. If not, a list of zeros is added as stimulus mask.\n",
    "    if current_layer in stimulus_layers:\n",
    "        G_exc.stimulus_mask = stimulus_mask\n",
    "    else:\n",
    "        G_exc.stimulus_mask = np.zeros(len(x))\n",
    "        \n",
    "    return G_exc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74232f22-77a9-4268-8595-d3a38f03a400",
   "metadata": {},
   "source": [
    "<br></br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b2a2423b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates the group of inhibitory neurons.\n",
    "def create_group_inh(topology, noise, treatment_mask, current_layer, treatment_layers, inhibitory_topologies_lengths, group_name='inh_group', integ_method='exponential_euler'):\n",
    "\n",
    "    # Extracting the passed on parameters.\n",
    "    x, y, z = topology\n",
    "    mu_noise, sigma_noise = noise\n",
    "\n",
    "    # Adjusting the group name.\n",
    "    group_name = group_name + \"_\" + current_layer\n",
    "\n",
    "    # Initializing a group of inhibitory neurons with the following parameters:\n",
    "    # - inh_eqs => Differential equations that define the behavior of the inhibitory neuron.\n",
    "    # - threshold => Neurons fire an action potential when their membrane potential 'v' exceeds the threshold 'V_th'.\n",
    "    # - refractory => Sets a refractory period during which an inhibitory neuron cannot fire again.\n",
    "    # - method => Specifies the integration method for solving the differential equations.\n",
    "    G_inh = NeuronGroup(len(x),inh_eqs,threshold='v>V_th', name=group_name, refractory=3*ms,method=integ_method)\n",
    "\n",
    "    # Initializing the membrane potential 'v' to be a random value between -60 mV and -70 mV.\n",
    "    G_inh.v = -60*mvolt-rand()*10*mvolt\n",
    "\n",
    "    # Sets the size of the neurons to 'taille_exc_normale'.\n",
    "    G_inh.taille = taille_inh_normale\n",
    "\n",
    "    # Assigns the positions of the neurons in micrometers.\n",
    "    G_inh.x = x * um\n",
    "    G_inh.y = y * um\n",
    "    G_inh.z = z * um\n",
    "\n",
    "    # Sets the mean and standard deviation of the noise affecting the neurons.\n",
    "    G_inh.mu_noise = mu_noise\n",
    "    G_inh.sigma_noise = sigma_noise\n",
    "\n",
    "    ### FOR THE MASKS: Optionally allow for multiple layers to be part of the treatment mask.\n",
    "    # Applying the treatment mask to the neuron by first checking whether the current layer is in the list of treatment layers. If not, a list of zeros is added as treatment mask.\n",
    "    if current_layer in treatment_layers:\n",
    "        G_inh.treatment_mask = treatment_mask\n",
    "    else:\n",
    "        G_inh.treatment_mask = np.zeros(len(x))\n",
    "\n",
    "    return G_inh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1857168-3caa-4f69-99eb-642b320a4f35",
   "metadata": {},
   "source": [
    "<br></br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "68b7a1c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates a group for Local Field Potential (LFP) recording.\n",
    "def create_group_lfp(LFP_electrode_layer, layer_geometrics):\n",
    "    \n",
    "    # Setting up a singular LFP electrode\n",
    "    Ne = 1\n",
    "\n",
    "    # Setting the resistivity of the extracellular field to 0.3 siemens per meter, which is within the typical range for biological tissue (0.3-0.4 S/m).\n",
    "    sigma = 0.3*siemens/meter\n",
    "\n",
    "    # Initializes a group of neurons, which will only consist of 1 representing a single LFP electrode which has three state variables:\n",
    "    # - v : volt => Represents the voltage (LFP signal).\n",
    "    # - x : meter => Represent the x-coordinate of the electrode.\n",
    "    # - y : meter => Represent the y-coordinate of the electrode.\n",
    "    # - z : meter => Represent the z-coordinate of the electrode.\n",
    "    lfp = NeuronGroup(Ne, model='''v : volt\n",
    "                                   x : meter\n",
    "                                   y : meter\n",
    "                                   z : meter''')\n",
    "\n",
    "    # Defining the x, y, and z coordinates of the LFP electrode based on what the layer the LFP electrode is in.\n",
    "    if LFP_electrode_layer == \"L1\":\n",
    "        lfp.x = (layer_geometrics['L1_block']['bottom_left_position'][0] + (layer_geometrics['L1_block']['width'] / 2)) * mm\n",
    "        lfp.y = (layer_geometrics['L1_block']['bottom_left_position'][1] + (layer_geometrics['L1_block']['height'] / 2)) * mm\n",
    "        lfp.z = (layer_geometrics['L1_block']['bottom_left_position'][2] + (layer_geometrics['L1_block']['depth'] / 2)) * mm\n",
    "    if LFP_electrode_layer == \"L2_L3\":\n",
    "        lfp.x = (layer_geometrics['L2_L3_block']['bottom_left_position'][0] + (layer_geometrics['L2_L3_block']['width'] / 2)) * mm\n",
    "        lfp.y = (layer_geometrics['L2_L3_block']['bottom_left_position'][1] + (layer_geometrics['L2_L3_block']['height'] / 2)) * mm\n",
    "        lfp.z = (layer_geometrics['L2_L3_block']['bottom_left_position'][2] + (layer_geometrics['L2_L3_block']['depth'] / 2)) * mm\n",
    "    if LFP_electrode_layer == \"L4\":\n",
    "        lfp.x = (layer_geometrics['L4_block']['bottom_left_position'][0] + (layer_geometrics['L4_block']['width'] / 2)) * mm\n",
    "        lfp.y = (layer_geometrics['L4_block']['bottom_left_position'][1] + (layer_geometrics['L4_block']['height'] / 2)) * mm\n",
    "        lfp.z = (layer_geometrics['L4_block']['bottom_left_position'][2] + (layer_geometrics['L4_block']['depth'] / 2)) * mm\n",
    "    if LFP_electrode_layer == \"L5\":\n",
    "        lfp.x = (layer_geometrics['L5_block']['bottom_left_position'][0] + (layer_geometrics['L5_block']['width'] / 2)) * mm\n",
    "        lfp.y = (layer_geometrics['L5_block']['bottom_left_position'][1] + (layer_geometrics['L5_block']['height'] / 2)) * mm\n",
    "        lfp.z = (layer_geometrics['L5_block']['bottom_left_position'][2] + (layer_geometrics['L5_block']['depth'] / 2)) * mm\n",
    "    if LFP_electrode_layer == \"L6\":\n",
    "        lfp.x = (layer_geometrics['L6_block']['bottom_left_position'][0] + (layer_geometrics['L6_block']['width'] / 2)) * mm\n",
    "        lfp.y = (layer_geometrics['L6_block']['bottom_left_position'][1] + (layer_geometrics['L6_block']['height'] / 2)) * mm\n",
    "        lfp.z = (layer_geometrics['L6_block']['bottom_left_position'][2] + (layer_geometrics['L6_block']['depth'] / 2)) * mm\n",
    "\n",
    "    return lfp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "558578b8-54f0-43fc-9247-653d15cf7106",
   "metadata": {},
   "source": [
    "<br></br><br></br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cb3ba69-edad-4d16-b306-97fd660d3c91",
   "metadata": {},
   "source": [
    "<h2 style=\"font-size: 40px;\">Network Configuration Function</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "644b49fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepares the network of neurons. The topology is already there, but the neurons still need to be connected, which is done here.\n",
    "def prepare_network(topologies, stimulus_mask_exc, treatment_masks, current_variables):\n",
    "    \n",
    "    # Extracting the connection probabilities between neurons from the same layer and from different layers.\n",
    "    connection_probabilities = current_variables['p']\n",
    "\n",
    "    # Extracting the topologies from the passed on parameter 'topologies'.\n",
    "    topology_L1_exc, topology_L2_L3_exc, topology_L4_exc, topology_L5_exc, topology_L6_exc, topology_L1_inh, topology_L2_L3_inh, topology_L4_inh, topology_L5_inh, topology_L6_inh = topologies\n",
    "\n",
    "    # Extracting the treatment masks from the passed on parameter 'treatment_masks'.\n",
    "    treatment_mask_exc, treatment_mask_inh = treatment_masks\n",
    "\n",
    "    # Calculating the lengths of the excitatory and inhibitory topologies.\n",
    "    excitatory_topologies_lengths = [len(topology_L1_exc[0]), len(topology_L2_L3_exc[0]), len(topology_L4_exc[0]), len(topology_L5_exc[0]), len(topology_L6_exc[0])]\n",
    "    inhibitory_topologies_lengths = [len(topology_L1_inh[0]), len(topology_L2_L3_inh[0]), len(topology_L4_inh[0]), len(topology_L5_inh[0]), len(topology_L6_inh[0])]\n",
    "\n",
    "    \n",
    "    ##########################################\n",
    "    ########  Creating Neuron Groups  ########\n",
    "    ##########################################\n",
    "    # Creating excitatory neuron groups for each layer.\n",
    "    G_L1_exc = create_group_py(topology_L1_exc, current_variables['noise_exc'], [stimulus_mask_exc, treatment_mask_exc], 'L1', current_variables['stimulus_layers'], current_variables['treatment_layers'], excitatory_topologies_lengths)\n",
    "    G_L2_L3_exc = create_group_py(topology_L2_L3_exc, current_variables['noise_exc'], [stimulus_mask_exc, treatment_mask_exc], 'L2_L3', current_variables['stimulus_layers'], current_variables['treatment_layers'], excitatory_topologies_lengths)\n",
    "    G_L4_exc = create_group_py(topology_L4_exc, current_variables['noise_exc'], [stimulus_mask_exc, treatment_mask_exc], 'L4', current_variables['stimulus_layers'], current_variables['treatment_layers'], excitatory_topologies_lengths)\n",
    "    G_L5_exc = create_group_py(topology_L5_exc, current_variables['noise_exc'], [stimulus_mask_exc, treatment_mask_exc], 'L5', current_variables['stimulus_layers'], current_variables['treatment_layers'], excitatory_topologies_lengths)\n",
    "    G_L6_exc = create_group_py(topology_L6_exc, current_variables['noise_exc'], [stimulus_mask_exc, treatment_mask_exc], 'L6', current_variables['stimulus_layers'], current_variables['treatment_layers'], excitatory_topologies_lengths)\n",
    "\n",
    "    # Creating inhibitory neuron groups for each layer.\n",
    "    G_L1_inh = create_group_inh(topology_L1_inh, current_variables['noise_inh'], treatment_mask_inh, 'L1', current_variables['treatment_layers'], inhibitory_topologies_lengths)\n",
    "    G_L2_L3_inh = create_group_inh(topology_L2_L3_inh, current_variables['noise_inh'], treatment_mask_inh, 'L2_L3', current_variables['treatment_layers'], inhibitory_topologies_lengths)\n",
    "    G_L4_inh = create_group_inh(topology_L4_inh, current_variables['noise_inh'], treatment_mask_inh, 'L4', current_variables['treatment_layers'], inhibitory_topologies_lengths)\n",
    "    G_L5_inh = create_group_inh(topology_L5_inh, current_variables['noise_inh'], treatment_mask_inh, 'L5', current_variables['treatment_layers'], inhibitory_topologies_lengths)\n",
    "    G_L6_inh = create_group_inh(topology_L6_inh, current_variables['noise_inh'], treatment_mask_inh, 'L6', current_variables['treatment_layers'], inhibitory_topologies_lengths)\n",
    "\n",
    "    # Setting up a Local Field Potential (LFP) electrode.\n",
    "    G_lfp = create_group_lfp(current_variables['LFP_electrode_layer'], current_variables['layer_geometrics'])\n",
    "    \n",
    "    # Adding the neuron groups to a list.\n",
    "    neuron_groups = [G_L1_exc, G_L2_L3_exc, G_L4_exc, G_L5_exc, G_L6_exc, G_L1_inh, G_L2_L3_inh, G_L4_inh, G_L5_inh, G_L6_inh, G_lfp]\n",
    "\n",
    "    \n",
    "    #####################################\n",
    "    ########  Creating Monitors  ########\n",
    "    #####################################\n",
    "    # Setting up monitors that monitor the population firing rate of all excitatory and inhibitory neuron groups.\n",
    "    popmon_L1_exc = PopulationRateMonitor(G_L1_exc)\n",
    "    popmon_L2_L3_exc = PopulationRateMonitor(G_L2_L3_exc)\n",
    "    popmon_L4_exc = PopulationRateMonitor(G_L4_exc)\n",
    "    popmon_L5_exc = PopulationRateMonitor(G_L5_exc)\n",
    "    popmon_L6_exc = PopulationRateMonitor(G_L6_exc)\n",
    "    popmon_L1_inh = PopulationRateMonitor(G_L1_inh)\n",
    "    popmon_L2_L3_inh = PopulationRateMonitor(G_L2_L3_inh)\n",
    "    popmon_L4_inh = PopulationRateMonitor(G_L4_inh)\n",
    "    popmon_L5_inh = PopulationRateMonitor(G_L5_inh)\n",
    "    popmon_L6_inh = PopulationRateMonitor(G_L6_inh)\n",
    "\n",
    "    # Setting up a monitor that monitors the voltage of the LFP group.\n",
    "    Mlfp = StateMonitor(G_lfp, 'v', record=True)\n",
    "\n",
    "    # Setting up monitors that monitor the membrane potential, the noise current (and the stimulus current) for the selected excitatory/inhibitory neurons.\n",
    "    statemon_L1_exc = StateMonitor(G_L1_exc, ('v', 'I_stim', 'I_noise'), record=[1,2,3,4,5,6], dt=0.001*second)\n",
    "    statemon_L2_L3_exc = StateMonitor(G_L2_L3_exc, ('v', 'I_stim', 'I_noise'), record=[1,2,3,4,5,6], dt=0.001*second)\n",
    "    statemon_L4_exc = StateMonitor(G_L4_exc, ('v', 'I_stim', 'I_noise'), record=[1,2,3,4,5,6], dt=0.001*second)\n",
    "    statemon_L5_exc = StateMonitor(G_L5_exc, ('v', 'I_stim', 'I_noise'), record=[1,2,3,4,5,6], dt=0.001*second)\n",
    "    statemon_L6_exc = StateMonitor(G_L6_exc, ('v', 'I_stim', 'I_noise'), record=[1,2,3,4,5,6], dt=0.001*second)\n",
    "    statemon_L1_inh = StateMonitor(G_L1_inh, ('v', 'I_noise'), record=[1,2,3,4,5,6], dt=0.001*second)\n",
    "    statemon_L2_L3_inh = StateMonitor(G_L2_L3_inh, ('v', 'I_noise'), record=[1,2,3,4,5,6], dt=0.001*second)\n",
    "    statemon_L4_inh = StateMonitor(G_L4_inh, ('v', 'I_noise'), record=[1,2,3,4,5,6], dt=0.001*second)\n",
    "    statemon_L5_inh = StateMonitor(G_L5_inh, ('v', 'I_noise'), record=[1,2,3,4,5,6], dt=0.001*second)\n",
    "    statemon_L6_inh = StateMonitor(G_L6_inh, ('v', 'I_noise'), record=[1,2,3,4,5,6], dt=0.001*second)\n",
    "    \n",
    "    monitors = [popmon_L1_exc, popmon_L2_L3_exc, popmon_L4_exc, popmon_L5_exc, popmon_L6_exc, popmon_L1_inh, popmon_L2_L3_inh, popmon_L4_inh, popmon_L5_inh, popmon_L6_inh, Mlfp, statemon_L1_exc, statemon_L2_L3_exc, statemon_L4_exc, statemon_L5_exc, statemon_L6_exc, statemon_L1_inh, statemon_L2_L3_inh, statemon_L4_inh, statemon_L5_inh, statemon_L6_inh]\n",
    "\n",
    "\n",
    "    ############################################\n",
    "    ########  Connecting Neuron Groups  ########\n",
    "    ############################################\n",
    "    # Configuring synaptic connections between neuron groups where in the function call 'Synapses()' the first parameter is the pre-synaptic group and the second parameter is the post-synaptic group. \n",
    "    # The third parameter 'on_pre' specifies the action to be taken when a pre-synaptic neuron fires where the synaptic event will increase the post-synaptic event by a certain amount defined by:\n",
    "    # - gain => A scaling factor for the synaptic strength.\n",
    "    # - g_max/siemens => The maximum conductance for the type of synapse normalized by dividing by 'siemens' which is a unit of conductance.\n",
    "    # - glu_pre => The amount of glutamate released from the pre-synaptic neuron upon firing.\n",
    "    # MIND: This does not create synapses but instead only specifies their dynamics. The actual synapse connections are created by using the function 'connect()'.\n",
    "    synapses = []\n",
    "    \n",
    "    ################################      \n",
    "    ## Connections Within layers ##\n",
    "    ################################\n",
    "    # Generating within the L1 layer the connections between the groups of neurons which is a probabilistic process indicating that not all neurons are connected with each other but instead it is decided based on the provided probability and a distance measure.\n",
    "    S_L1_e2e = Synapses(G_L1_exc, G_L1_exc, on_pre=\"he_post+=\"+str(gain)+\"*\"+str(g_max_e/siemens)+\"*siemens*glu_pre\", name='synapses_within_G_L1_exc_e2e')\n",
    "    S_L1_e2i = Synapses(G_L1_exc, G_L1_inh, on_pre=\"he_post+=\"+str(gain)+\"*\"+str(g_max_e/siemens)+\"*siemens*glu_pre\", name='synapses_within_G_L1_exc_e2i')\n",
    "    S_L1_i2e = Synapses(G_L1_inh, G_L1_exc, on_pre=\"hi_post+=\"+str(gain)+\"*\"+str(g_max_i/siemens)+\"*siemens\", name='synapses_within_G_L1_exc_i2e')\n",
    "    S_L1_i2i = Synapses(G_L1_inh, G_L1_inh, on_pre=\"hi_post+=\"+str(gain)+\"*\"+str(g_max_i/siemens)+\"*siemens\", name='synapses_within_G_L1_exc_i2i')\n",
    "    S_L1_e2e.connect(p=f'{current_variables['p'][0]}*exp(-(((x_pre-x_post)**2+(y_pre-y_post)**2+(z_pre-z_post)**2) / (({489.9}*umeter)**2)))')\n",
    "    S_L1_e2i.connect(p=f'{current_variables['p'][1]}*exp(-(((x_pre-x_post)**2+(y_pre-y_post)**2+(z_pre-z_post)**2) / (({489.9}*umeter)**2)))')\n",
    "    S_L1_i2e.connect(p=f'{current_variables['p'][2]}*exp(-(((x_pre-x_post)**2+(y_pre-y_post)**2+(z_pre-z_post)**2) / (({489.9}*umeter)**2)))')\n",
    "    S_L1_i2i.connect(p=f'{current_variables['p'][3]}*exp(-(((x_pre-x_post)**2+(y_pre-y_post)**2+(z_pre-z_post)**2) / (({489.9}*umeter)**2)))')\n",
    "    synapses.append(S_L1_e2e)\n",
    "    synapses.append(S_L1_e2i)\n",
    "    synapses.append(S_L1_i2e)\n",
    "    synapses.append(S_L1_i2i)\n",
    "\n",
    "    # Generating within the L2_L3 layer the connections between the groups of neurons which is a probabilistic process indicating that not all neurons are connected with each other but instead it is decided based on the provided probability and a distance measure.\n",
    "    S_L2_L3_e2e = Synapses(G_L2_L3_exc, G_L2_L3_exc, on_pre=\"he_post+=\"+str(gain)+\"*\"+str(g_max_e/siemens)+\"*siemens*glu_pre\", name='synapses_within_G_L2_L3_exc_e2e')\n",
    "    S_L2_L3_e2i = Synapses(G_L2_L3_exc, G_L2_L3_inh, on_pre=\"he_post+=\"+str(gain)+\"*\"+str(g_max_e/siemens)+\"*siemens*glu_pre\", name='synapses_within_G_L2_L3_exc_e2i')\n",
    "    S_L2_L3_i2e = Synapses(G_L2_L3_inh, G_L2_L3_exc, on_pre=\"hi_post+=\"+str(gain)+\"*\"+str(g_max_i/siemens)+\"*siemens\", name='synapses_within_G_L2_L3_exc_i2e')\n",
    "    S_L2_L3_i2i = Synapses(G_L2_L3_inh, G_L2_L3_inh, on_pre=\"hi_post+=\"+str(gain)+\"*\"+str(g_max_i/siemens)+\"*siemens\", name='synapses_within_G_L2_L3_exc_i2i')\n",
    "    S_L2_L3_e2e.connect(p=f'{current_variables['p'][0]}*exp(-(((x_pre-x_post)**2+(y_pre-y_post)**2+(z_pre-z_post)**2) / (({489.9}*umeter)**2)))')\n",
    "    S_L2_L3_e2i.connect(p=f'{current_variables['p'][1]}*exp(-(((x_pre-x_post)**2+(y_pre-y_post)**2+(z_pre-z_post)**2) / (({489.9}*umeter)**2)))')\n",
    "    S_L2_L3_i2e.connect(p=f'{current_variables['p'][2]}*exp(-(((x_pre-x_post)**2+(y_pre-y_post)**2+(z_pre-z_post)**2) / (({489.9}*umeter)**2)))')\n",
    "    S_L2_L3_i2i.connect(p=f'{current_variables['p'][3]}*exp(-(((x_pre-x_post)**2+(y_pre-y_post)**2+(z_pre-z_post)**2) / (({489.9}*umeter)**2)))')\n",
    "    synapses.append(S_L2_L3_e2e)\n",
    "    synapses.append(S_L2_L3_e2i)\n",
    "    synapses.append(S_L2_L3_i2e)\n",
    "    synapses.append(S_L2_L3_i2i)\n",
    "\n",
    "    # Generating within the L4 layer the connections between the groups of neurons which is a probabilistic process indicating that not all neurons are connected with each other but instead it is decided based on the provided probability and a distance measure.\n",
    "    S_L4_e2e = Synapses(G_L4_exc, G_L4_exc, on_pre=\"he_post+=\"+str(gain)+\"*\"+str(g_max_e/siemens)+\"*siemens*glu_pre\", name='synapses_within_G_L4_exc_e2e')\n",
    "    S_L4_e2i = Synapses(G_L4_exc, G_L4_inh, on_pre=\"he_post+=\"+str(gain)+\"*\"+str(g_max_e/siemens)+\"*siemens*glu_pre\", name='synapses_within_G_L4_exc_e2i')\n",
    "    S_L4_i2e = Synapses(G_L4_inh, G_L4_exc, on_pre=\"hi_post+=\"+str(gain)+\"*\"+str(g_max_i/siemens)+\"*siemens\", name='synapses_within_G_L4_exc_i2e')\n",
    "    S_L4_i2i = Synapses(G_L4_inh, G_L4_inh, on_pre=\"hi_post+=\"+str(gain)+\"*\"+str(g_max_i/siemens)+\"*siemens\", name='synapses_within_G_L4_exc_i2i')\n",
    "    S_L4_e2e.connect(p=f'{current_variables['p'][0]}*exp(-(((x_pre-x_post)**2+(y_pre-y_post)**2+(z_pre-z_post)**2) / (({489.9}*umeter)**2)))')\n",
    "    S_L4_e2i.connect(p=f'{current_variables['p'][1]}*exp(-(((x_pre-x_post)**2+(y_pre-y_post)**2+(z_pre-z_post)**2) / (({489.9}*umeter)**2)))')\n",
    "    S_L4_i2e.connect(p=f'{current_variables['p'][2]}*exp(-(((x_pre-x_post)**2+(y_pre-y_post)**2+(z_pre-z_post)**2) / (({489.9}*umeter)**2)))')\n",
    "    S_L4_i2i.connect(p=f'{current_variables['p'][3]}*exp(-(((x_pre-x_post)**2+(y_pre-y_post)**2+(z_pre-z_post)**2) / (({489.9}*umeter)**2)))')\n",
    "    synapses.append(S_L4_e2e)\n",
    "    synapses.append(S_L4_e2i)\n",
    "    synapses.append(S_L4_i2e)\n",
    "    synapses.append(S_L4_i2i)\n",
    "\n",
    "    # Generating within the L5 layer the connections between the groups of neurons which is a probabilistic process indicating that not all neurons are connected with each other but instead it is decided based on the provided probability and a distance measure.\n",
    "    S_L5_e2e = Synapses(G_L5_exc, G_L5_exc, on_pre=\"he_post+=\"+str(gain)+\"*\"+str(g_max_e/siemens)+\"*siemens*glu_pre\", name='synapses_within_G_L5_exc_e2e')\n",
    "    S_L5_e2i = Synapses(G_L5_exc, G_L5_inh, on_pre=\"he_post+=\"+str(gain)+\"*\"+str(g_max_e/siemens)+\"*siemens*glu_pre\", name='synapses_within_G_L5_exc_e2i')\n",
    "    S_L5_i2e = Synapses(G_L5_inh, G_L5_exc, on_pre=\"hi_post+=\"+str(gain)+\"*\"+str(g_max_i/siemens)+\"*siemens\", name='synapses_within_G_L5_exc_i2e')\n",
    "    S_L5_i2i = Synapses(G_L5_inh, G_L5_inh, on_pre=\"hi_post+=\"+str(gain)+\"*\"+str(g_max_i/siemens)+\"*siemens\", name='synapses_within_G_L5_exc_i2i')\n",
    "    S_L5_e2e.connect(p=f'{current_variables['p'][0]}*exp(-(((x_pre-x_post)**2+(y_pre-y_post)**2+(z_pre-z_post)**2) / (({489.9}*umeter)**2)))')\n",
    "    S_L5_e2i.connect(p=f'{current_variables['p'][1]}*exp(-(((x_pre-x_post)**2+(y_pre-y_post)**2+(z_pre-z_post)**2) / (({489.9}*umeter)**2)))')\n",
    "    S_L5_i2e.connect(p=f'{current_variables['p'][2]}*exp(-(((x_pre-x_post)**2+(y_pre-y_post)**2+(z_pre-z_post)**2) / (({489.9}*umeter)**2)))')\n",
    "    S_L5_i2i.connect(p=f'{current_variables['p'][3]}*exp(-(((x_pre-x_post)**2+(y_pre-y_post)**2+(z_pre-z_post)**2) / (({489.9}*umeter)**2)))')\n",
    "    synapses.append(S_L5_e2e)\n",
    "    synapses.append(S_L5_e2i)\n",
    "    synapses.append(S_L5_i2e)\n",
    "    synapses.append(S_L5_i2i)\n",
    "\n",
    "    # Generating within the L6 layer the connections between the groups of neurons which is a probabilistic process indicating that not all neurons are connected with each other but instead it is decided based on the provided probability and a distance measure.\n",
    "    S_L6_e2e = Synapses(G_L6_exc, G_L6_exc, on_pre=\"he_post+=\"+str(gain)+\"*\"+str(g_max_e/siemens)+\"*siemens*glu_pre\", name='synapses_within_G_L6_exc_e2e')\n",
    "    S_L6_e2i = Synapses(G_L6_exc, G_L6_inh, on_pre=\"he_post+=\"+str(gain)+\"*\"+str(g_max_e/siemens)+\"*siemens*glu_pre\", name='synapses_within_G_L6_exc_e2i')\n",
    "    S_L6_i2e = Synapses(G_L6_inh, G_L6_exc, on_pre=\"hi_post+=\"+str(gain)+\"*\"+str(g_max_i/siemens)+\"*siemens\", name='synapses_within_G_L6_exc_i2e')\n",
    "    S_L6_i2i = Synapses(G_L6_inh, G_L6_inh, on_pre=\"hi_post+=\"+str(gain)+\"*\"+str(g_max_i/siemens)+\"*siemens\", name='synapses_within_G_L6_exc_i2i')\n",
    "    S_L6_e2e.connect(p=f'{current_variables['p'][0]}*exp(-(((x_pre-x_post)**2+(y_pre-y_post)**2+(z_pre-z_post)**2) / (({489.9}*umeter)**2)))')\n",
    "    S_L6_e2i.connect(p=f'{current_variables['p'][1]}*exp(-(((x_pre-x_post)**2+(y_pre-y_post)**2+(z_pre-z_post)**2) / (({489.9}*umeter)**2)))')\n",
    "    S_L6_i2e.connect(p=f'{current_variables['p'][2]}*exp(-(((x_pre-x_post)**2+(y_pre-y_post)**2+(z_pre-z_post)**2) / (({489.9}*umeter)**2)))')\n",
    "    S_L6_i2i.connect(p=f'{current_variables['p'][3]}*exp(-(((x_pre-x_post)**2+(y_pre-y_post)**2+(z_pre-z_post)**2) / (({489.9}*umeter)**2)))')\n",
    "    synapses.append(S_L6_e2e)\n",
    "    synapses.append(S_L6_e2i)\n",
    "    synapses.append(S_L6_i2e)\n",
    "    synapses.append(S_L6_i2i)\n",
    "\n",
    "\n",
    "\n",
    "    #################################      \n",
    "    ## Connections Between layers ##\n",
    "    #################################\n",
    "    # Generating from the L1 to the L2_L3 layer the connections between the groups of neurons which is a probabilistic process indicating that not all neurons are connected with each other but instead it is decided based on the provided probability and a distance measure.\n",
    "    S_L1_L2_L3_e2e = Synapses(G_L1_exc, G_L2_L3_exc, on_pre=\"he_post+=\"+str(gain)+\"*\"+str(g_max_e/siemens)+\"*siemens*glu_pre\", name='synapses_between_G_L1_and_L2_L3_exc_e2e')\n",
    "    S_L1_L2_L3_e2i = Synapses(G_L1_exc, G_L2_L3_inh, on_pre=\"he_post+=\"+str(gain)+\"*\"+str(g_max_e/siemens)+\"*siemens*glu_pre\", name='synapses_between_G_L1_and_L2_L3_exc_e2i')\n",
    "    S_L1_L2_L3_i2e = Synapses(G_L1_inh, G_L2_L3_exc, on_pre=\"hi_post+=\"+str(gain)+\"*\"+str(g_max_i/siemens)+\"*siemens\", name='synapses_between_G_L1_and_L2_L3_exc_i2e')\n",
    "    S_L1_L2_L3_i2i = Synapses(G_L1_inh, G_L2_L3_inh, on_pre=\"hi_post+=\"+str(gain)+\"*\"+str(g_max_i/siemens)+\"*siemens\", name='synapses_between_G_L1_and_L2_L3_exc_i2i')\n",
    "    S_L1_L2_L3_e2e.connect(p=f'{current_variables['p'][0]}*exp(-(((x_pre-x_post)**2+(y_pre-y_post)**2+(z_pre-z_post)**2) / (({489.9}*umeter)**2)))')\n",
    "    S_L1_L2_L3_e2i.connect(p=f'{current_variables['p'][1]}*exp(-(((x_pre-x_post)**2+(y_pre-y_post)**2+(z_pre-z_post)**2) / (({489.9}*umeter)**2)))')\n",
    "    S_L1_L2_L3_i2e.connect(p=f'{current_variables['p'][2]}*exp(-(((x_pre-x_post)**2+(y_pre-y_post)**2+(z_pre-z_post)**2) / (({489.9}*umeter)**2)))')\n",
    "    S_L1_L2_L3_i2i.connect(p=f'{current_variables['p'][3]}*exp(-(((x_pre-x_post)**2+(y_pre-y_post)**2+(z_pre-z_post)**2) / (({489.9}*umeter)**2)))')\n",
    "    synapses.append(S_L1_L2_L3_e2e)\n",
    "    synapses.append(S_L1_L2_L3_e2i)\n",
    "    synapses.append(S_L1_L2_L3_i2e)\n",
    "    synapses.append(S_L1_L2_L3_i2i)\n",
    "\n",
    "    # Generating from the L4 to the L2_L3 layer the connections between the groups of neurons which is a probabilistic process indicating that not all neurons are connected with each other but instead it is decided based on the provided probability and a distance measure.\n",
    "    S_L4_L2_L3_e2e = Synapses(G_L4_exc, G_L2_L3_exc, on_pre=\"he_post+=\"+str(gain)+\"*\"+str(g_max_e/siemens)+\"*siemens*glu_pre\", name='synapses_between_G_L4_and_L2_L3_exc_e2e')\n",
    "    S_L4_L2_L3_e2i = Synapses(G_L4_exc, G_L2_L3_inh, on_pre=\"he_post+=\"+str(gain)+\"*\"+str(g_max_e/siemens)+\"*siemens*glu_pre\", name='synapses_between_G_L4_and_L2_L3_exc_e2i')\n",
    "    S_L4_L2_L3_i2e = Synapses(G_L4_inh, G_L2_L3_exc, on_pre=\"hi_post+=\"+str(gain)+\"*\"+str(g_max_i/siemens)+\"*siemens\", name='synapses_between_G_L4_and_L2_L3_exc_i2e')\n",
    "    S_L4_L2_L3_i2i = Synapses(G_L4_inh, G_L2_L3_inh, on_pre=\"hi_post+=\"+str(gain)+\"*\"+str(g_max_i/siemens)+\"*siemens\", name='synapses_between_G_L4_and_L2_L3_exc_i2i')\n",
    "    S_L4_L2_L3_e2e.connect(p=f'{current_variables['p'][0]}*exp(-(((x_pre-x_post)**2+(y_pre-y_post)**2+(z_pre-z_post)**2) / (({489.9}*umeter)**2)))')\n",
    "    S_L4_L2_L3_e2i.connect(p=f'{current_variables['p'][1]}*exp(-(((x_pre-x_post)**2+(y_pre-y_post)**2+(z_pre-z_post)**2) / (({489.9}*umeter)**2)))')\n",
    "    S_L4_L2_L3_i2e.connect(p=f'{current_variables['p'][2]}*exp(-(((x_pre-x_post)**2+(y_pre-y_post)**2+(z_pre-z_post)**2) / (({489.9}*umeter)**2)))')\n",
    "    S_L4_L2_L3_i2i.connect(p=f'{current_variables['p'][3]}*exp(-(((x_pre-x_post)**2+(y_pre-y_post)**2+(z_pre-z_post)**2) / (({489.9}*umeter)**2)))')\n",
    "    synapses.append(S_L4_L2_L3_e2e)\n",
    "    synapses.append(S_L4_L2_L3_e2i)\n",
    "    synapses.append(S_L4_L2_L3_i2e)\n",
    "    synapses.append(S_L4_L2_L3_i2i)\n",
    "\n",
    "    # Generating from the L5 to the L6 layer the connections between the groups of neurons which is a probabilistic process indicating that not all neurons are connected with each other but instead it is decided based on the provided probability and a distance measure.\n",
    "    S_L5_L6_e2e = Synapses(G_L5_exc, G_L6_exc, on_pre=\"he_post+=\"+str(gain)+\"*\"+str(g_max_e/siemens)+\"*siemens*glu_pre\", name='synapses_between_G_L5_and_L6_exc_e2e')\n",
    "    S_L5_L6_e2i = Synapses(G_L5_exc, G_L6_inh, on_pre=\"he_post+=\"+str(gain)+\"*\"+str(g_max_e/siemens)+\"*siemens*glu_pre\", name='synapses_between_G_L5_and_L6_exc_e2i')\n",
    "    S_L5_L6_i2e = Synapses(G_L5_inh, G_L6_exc, on_pre=\"hi_post+=\"+str(gain)+\"*\"+str(g_max_i/siemens)+\"*siemens\", name='synapses_between_G_L5_and_L6_exc_i2e')\n",
    "    S_L5_L6_i2i = Synapses(G_L5_inh, G_L6_inh, on_pre=\"hi_post+=\"+str(gain)+\"*\"+str(g_max_i/siemens)+\"*siemens\", name='synapses_between_G_L5_and_L6_exc_i2i')\n",
    "    S_L5_L6_e2e.connect(p=f'{current_variables['p'][0]}*exp(-(((x_pre-x_post)**2+(y_pre-y_post)**2+(z_pre-z_post)**2) / (({489.9}*umeter)**2)))')\n",
    "    S_L5_L6_e2i.connect(p=f'{current_variables['p'][1]}*exp(-(((x_pre-x_post)**2+(y_pre-y_post)**2+(z_pre-z_post)**2) / (({489.9}*umeter)**2)))')\n",
    "    S_L5_L6_i2e.connect(p=f'{current_variables['p'][2]}*exp(-(((x_pre-x_post)**2+(y_pre-y_post)**2+(z_pre-z_post)**2) / (({489.9}*umeter)**2)))')\n",
    "    S_L5_L6_i2i.connect(p=f'{current_variables['p'][3]}*exp(-(((x_pre-x_post)**2+(y_pre-y_post)**2+(z_pre-z_post)**2) / (({489.9}*umeter)**2)))')\n",
    "    synapses.append(S_L5_L6_e2e)\n",
    "    synapses.append(S_L5_L6_e2i)\n",
    "    synapses.append(S_L5_L6_i2e)\n",
    "    synapses.append(S_L5_L6_i2i)\n",
    "\n",
    "    \n",
    "    # Since we want the total number of connections from the L1 to the L5 layer to be equal when only using the given probability versus when using both the given probability and distance measure, we can examine how many connections are made for each case.\n",
    "    # This is done separately for all four possible combinations of neuron type connections.\n",
    "    testing_S_L1_L5_e2e_no_dist = Synapses(G_L1_exc, G_L5_exc, on_pre=\"he_post+=\"+str(gain)+\"*\"+str(g_max_e/siemens)+\"*siemens*glu_pre\")\n",
    "    testing_S_L1_L5_e2e_dist = Synapses(G_L1_exc, G_L5_exc, on_pre=\"he_post+=\"+str(gain)+\"*\"+str(g_max_e/siemens)+\"*siemens*glu_pre\")\n",
    "    testing_S_L1_L5_e2e_no_dist.connect(p=current_variables['p'][0])\n",
    "    testing_S_L1_L5_e2e_dist.connect(p=f'{current_variables['p'][0]}*exp(-(((x_pre-x_post)**2+(y_pre-y_post)**2+(z_pre-z_post)**2) / (({489.9}*umeter)**2)))')\n",
    "    ratio_L1_L5_e2e = len(testing_S_L1_L5_e2e_dist.N_outgoing_pre) / len(testing_S_L1_L5_e2e_no_dist.N_outgoing_pre)\n",
    "    \n",
    "    testing_S_L1_L5_e2i_no_dist = Synapses(G_L1_exc, G_L5_inh, on_pre=\"he_post+=\"+str(gain)+\"*\"+str(g_max_e/siemens)+\"*siemens*glu_pre\")\n",
    "    testing_S_L1_L5_e2i_dist = Synapses(G_L1_exc, G_L5_inh, on_pre=\"he_post+=\"+str(gain)+\"*\"+str(g_max_e/siemens)+\"*siemens*glu_pre\")\n",
    "    testing_S_L1_L5_e2i_no_dist.connect(p=current_variables['p'][1])\n",
    "    testing_S_L1_L5_e2i_dist.connect(p=f'{current_variables['p'][1]}*exp(-(((x_pre-x_post)**2+(y_pre-y_post)**2+(z_pre-z_post)**2) / (({489.9}*umeter)**2)))')\n",
    "    ratio_L1_L5_e2i = len(testing_S_L1_L5_e2i_dist.N_outgoing_pre) / len(testing_S_L1_L5_e2i_no_dist.N_outgoing_pre)\n",
    "\n",
    "    testing_S_L1_L5_i2e_no_dist = Synapses(G_L1_inh, G_L5_exc, on_pre=\"hi_post+=\"+str(gain)+\"*\"+str(g_max_i/siemens)+\"*siemens\")\n",
    "    testing_S_L1_L5_i2e_dist = Synapses(G_L1_inh, G_L5_exc, on_pre=\"hi_post+=\"+str(gain)+\"*\"+str(g_max_i/siemens)+\"*siemens\")\n",
    "    testing_S_L1_L5_i2e_no_dist.connect(p=current_variables['p'][2])\n",
    "    testing_S_L1_L5_i2e_dist.connect(p=f'{current_variables['p'][2]}*exp(-(((x_pre-x_post)**2+(y_pre-y_post)**2+(z_pre-z_post)**2) / (({489.9}*umeter)**2)))')\n",
    "    ratio_L1_L5_i2e = len(testing_S_L1_L5_i2e_dist.N_outgoing_pre) / len(testing_S_L1_L5_i2e_no_dist.N_outgoing_pre)\n",
    "\n",
    "    testing_S_L1_L5_i2i_no_dist = Synapses(G_L1_inh, G_L5_inh, on_pre=\"hi_post+=\"+str(gain)+\"*\"+str(g_max_i/siemens)+\"*siemens\")\n",
    "    testing_S_L1_L5_i2i_dist = Synapses(G_L1_inh, G_L5_inh, on_pre=\"hi_post+=\"+str(gain)+\"*\"+str(g_max_i/siemens)+\"*siemens\")\n",
    "    testing_S_L1_L5_i2i_no_dist.connect(p=current_variables['p'][3])\n",
    "    testing_S_L1_L5_i2i_dist.connect(p=f'{current_variables['p'][3]}*exp(-(((x_pre-x_post)**2+(y_pre-y_post)**2+(z_pre-z_post)**2) / (({489.9}*umeter)**2)))')\n",
    "    ratio_L1_L5_i2i = len(testing_S_L1_L5_i2i_dist.N_outgoing_pre) / len(testing_S_L1_L5_i2i_no_dist.N_outgoing_pre)\n",
    "\n",
    "    # These ratios can be used to multiply with the given probability in the 'connect()' function to ensure that the same number of connections would have been made if the distance measure was also included in the probability.\n",
    "    S_L1_L5_e2e = Synapses(G_L1_exc, G_L5_exc, on_pre=\"he_post+=\"+str(gain)+\"*\"+str(g_max_e/siemens)+\"*siemens*glu_pre\", name='synapses_between_L1_L5_e2e')\n",
    "    S_L1_L5_e2i = Synapses(G_L1_exc, G_L5_inh, on_pre=\"he_post+=\"+str(gain)+\"*\"+str(g_max_e/siemens)+\"*siemens*glu_pre\", name='synapses_between_L1_L5_e2i')\n",
    "    S_L1_L5_i2e = Synapses(G_L1_inh, G_L5_exc, on_pre=\"hi_post+=\"+str(gain)+\"*\"+str(g_max_i/siemens)+\"*siemens\", name='synapses_between_L1_L5_i2e')\n",
    "    S_L1_L5_i2i = Synapses(G_L1_inh, G_L5_inh, on_pre=\"hi_post+=\"+str(gain)+\"*\"+str(g_max_i/siemens)+\"*siemens\", name='synapses_between_L1_L5_i2i')\n",
    "    S_L1_L5_e2e.connect(p=current_variables['p'][0]*ratio_L1_L5_e2e)\n",
    "    S_L1_L5_e2i.connect(p=current_variables['p'][1]*ratio_L1_L5_e2i)\n",
    "    S_L1_L5_i2e.connect(p=current_variables['p'][2]*ratio_L1_L5_i2e)\n",
    "    S_L1_L5_i2i.connect(p=current_variables['p'][3]*ratio_L1_L5_i2i)\n",
    "    synapses.append(S_L1_L5_e2e)\n",
    "    synapses.append(S_L1_L5_e2i)\n",
    "    synapses.append(S_L1_L5_i2e)\n",
    "    synapses.append(S_L1_L5_i2i)\n",
    "\n",
    "\n",
    "    # Since we want the total number of connections from the L2_L3 to the L5 layer to be equal when only using the given probability versus when using both the given probability and distance measure, we can examine how many connections are made for each case.\n",
    "    # This is done separately for all four possible combinations of neuron type connections.\n",
    "    testing_S_L2_L3_L5_e2e_no_dist = Synapses(G_L2_L3_exc, G_L5_exc, on_pre=\"he_post+=\"+str(gain)+\"*\"+str(g_max_e/siemens)+\"*siemens*glu_pre\")\n",
    "    testing_S_L2_L3_L5_e2e_dist = Synapses(G_L2_L3_exc, G_L5_exc, on_pre=\"he_post+=\"+str(gain)+\"*\"+str(g_max_e/siemens)+\"*siemens*glu_pre\")\n",
    "    testing_S_L2_L3_L5_e2e_no_dist.connect(p=current_variables['p'][0])\n",
    "    testing_S_L2_L3_L5_e2e_dist.connect(p=f'{current_variables['p'][0]}*exp(-(((x_pre-x_post)**2+(y_pre-y_post)**2+(z_pre-z_post)**2) / (({489.9}*umeter)**2)))')\n",
    "    ratio_L2_L3_L5_e2e = len(testing_S_L2_L3_L5_e2e_dist.N_outgoing_pre) / len(testing_S_L2_L3_L5_e2e_no_dist.N_outgoing_pre)\n",
    "    \n",
    "    testing_S_L2_L3_L5_e2i_no_dist = Synapses(G_L2_L3_exc, G_L5_inh, on_pre=\"he_post+=\"+str(gain)+\"*\"+str(g_max_e/siemens)+\"*siemens*glu_pre\")\n",
    "    testing_S_L2_L3_L5_e2i_dist = Synapses(G_L2_L3_exc, G_L5_inh, on_pre=\"he_post+=\"+str(gain)+\"*\"+str(g_max_e/siemens)+\"*siemens*glu_pre\")\n",
    "    testing_S_L2_L3_L5_e2i_no_dist.connect(p=current_variables['p'][1])\n",
    "    testing_S_L2_L3_L5_e2i_dist.connect(p=f'{current_variables['p'][1]}*exp(-(((x_pre-x_post)**2+(y_pre-y_post)**2+(z_pre-z_post)**2) / (({489.9}*umeter)**2)))')\n",
    "    ratio_L2_L3_L5_e2i = len(testing_S_L2_L3_L5_e2i_dist.N_outgoing_pre) / len(testing_S_L2_L3_L5_e2i_no_dist.N_outgoing_pre)\n",
    "\n",
    "    testing_S_L2_L3_L5_i2e_no_dist = Synapses(G_L2_L3_inh, G_L5_exc, on_pre=\"hi_post+=\"+str(gain)+\"*\"+str(g_max_i/siemens)+\"*siemens\")\n",
    "    testing_S_L2_L3_L5_i2e_dist = Synapses(G_L2_L3_inh, G_L5_exc, on_pre=\"hi_post+=\"+str(gain)+\"*\"+str(g_max_i/siemens)+\"*siemens\")\n",
    "    testing_S_L2_L3_L5_i2e_no_dist.connect(p=current_variables['p'][2])\n",
    "    testing_S_L2_L3_L5_i2e_dist.connect(p=f'{current_variables['p'][2]}*exp(-(((x_pre-x_post)**2+(y_pre-y_post)**2+(z_pre-z_post)**2) / (({489.9}*umeter)**2)))')\n",
    "    ratio_L2_L3_L5_i2e = len(testing_S_L2_L3_L5_i2e_dist.N_outgoing_pre) / len(testing_S_L2_L3_L5_i2e_no_dist.N_outgoing_pre)\n",
    "\n",
    "    testing_S_L2_L3_L5_i2i_no_dist = Synapses(G_L2_L3_inh, G_L5_inh, on_pre=\"hi_post+=\"+str(gain)+\"*\"+str(g_max_i/siemens)+\"*siemens\")\n",
    "    testing_S_L2_L3_L5_i2i_dist = Synapses(G_L2_L3_inh, G_L5_inh, on_pre=\"hi_post+=\"+str(gain)+\"*\"+str(g_max_i/siemens)+\"*siemens\")\n",
    "    testing_S_L2_L3_L5_i2i_no_dist.connect(p=current_variables['p'][3])\n",
    "    testing_S_L2_L3_L5_i2i_dist.connect(p=f'{current_variables['p'][3]}*exp(-(((x_pre-x_post)**2+(y_pre-y_post)**2+(z_pre-z_post)**2) / (({489.9}*umeter)**2)))')\n",
    "    ratio_L2_L3_L5_i2i = len(testing_S_L2_L3_L5_i2i_dist.N_outgoing_pre) / len(testing_S_L2_L3_L5_i2i_no_dist.N_outgoing_pre)\n",
    "\n",
    "    # These ratios can be used to multiply with the given probability in the 'connect()' function to ensure that the same number of connections would have been made if the distance measure was also included in the probability.\n",
    "    S_L2_L3_L5_e2e = Synapses(G_L2_L3_exc, G_L5_exc, on_pre=\"he_post+=\"+str(gain)+\"*\"+str(g_max_e/siemens)+\"*siemens*glu_pre\", name='synapses_between_L2_L3_L5_e2e')\n",
    "    S_L2_L3_L5_e2i = Synapses(G_L2_L3_exc, G_L5_inh, on_pre=\"he_post+=\"+str(gain)+\"*\"+str(g_max_e/siemens)+\"*siemens*glu_pre\", name='synapses_between_L2_L3_L5_e2i')\n",
    "    S_L2_L3_L5_i2e = Synapses(G_L2_L3_inh, G_L5_exc, on_pre=\"hi_post+=\"+str(gain)+\"*\"+str(g_max_i/siemens)+\"*siemens\", name='synapses_between_L2_L3_L5_i2e')\n",
    "    S_L2_L3_L5_i2i = Synapses(G_L2_L3_inh, G_L5_inh, on_pre=\"hi_post+=\"+str(gain)+\"*\"+str(g_max_i/siemens)+\"*siemens\", name='synapses_between_L2_L3_L5_i2i')\n",
    "    S_L2_L3_L5_e2e.connect(p=current_variables['p'][0]*ratio_L2_L3_L5_e2e)\n",
    "    S_L2_L3_L5_e2i.connect(p=current_variables['p'][1]*ratio_L2_L3_L5_e2i)\n",
    "    S_L2_L3_L5_i2e.connect(p=current_variables['p'][2]*ratio_L2_L3_L5_i2e)\n",
    "    S_L2_L3_L5_i2i.connect(p=current_variables['p'][3]*ratio_L2_L3_L5_i2i)\n",
    "    synapses.append(S_L2_L3_L5_e2e)\n",
    "    synapses.append(S_L2_L3_L5_e2i)\n",
    "    synapses.append(S_L2_L3_L5_i2e)\n",
    "    synapses.append(S_L2_L3_L5_i2i)\n",
    "\n",
    "\n",
    "    # Since we want the total number of connections from the L4 to the L2_L3 layer to be equal when only using the given probability versus when using both the given probability and distance measure, we can examine how many connections are made for each case.\n",
    "    # This is done separately for all four possible combinations of neuron type connections.\n",
    "    testing_S_L4_L2_L3_e2e_no_dist = Synapses(G_L4_exc, G_L2_L3_exc, on_pre=\"he_post+=\"+str(gain)+\"*\"+str(g_max_e/siemens)+\"*siemens*glu_pre\")\n",
    "    testing_S_L4_L2_L3_e2e_dist = Synapses(G_L4_exc, G_L2_L3_exc, on_pre=\"he_post+=\"+str(gain)+\"*\"+str(g_max_e/siemens)+\"*siemens*glu_pre\")\n",
    "    testing_S_L4_L2_L3_e2e_no_dist.connect(p=current_variables['p'][0])\n",
    "    testing_S_L4_L2_L3_e2e_dist.connect(p=f'{current_variables['p'][0]}*exp(-(((x_pre-x_post)**2+(y_pre-y_post)**2+(z_pre-z_post)**2) / (({489.9}*umeter)**2)))')\n",
    "    ratio_L4_L2_L3_e2e = len(testing_S_L4_L2_L3_e2e_dist.N_outgoing_pre) / len(testing_S_L4_L2_L3_e2e_no_dist.N_outgoing_pre)\n",
    "    \n",
    "    testing_S_L4_L2_L3_e2i_no_dist = Synapses(G_L4_exc, G_L2_L3_inh, on_pre=\"he_post+=\"+str(gain)+\"*\"+str(g_max_e/siemens)+\"*siemens*glu_pre\")\n",
    "    testing_S_L4_L2_L3_e2i_dist = Synapses(G_L4_exc, G_L2_L3_inh, on_pre=\"he_post+=\"+str(gain)+\"*\"+str(g_max_e/siemens)+\"*siemens*glu_pre\")\n",
    "    testing_S_L4_L2_L3_e2i_no_dist.connect(p=current_variables['p'][1])\n",
    "    testing_S_L4_L2_L3_e2i_dist.connect(p=f'{current_variables['p'][1]}*exp(-(((x_pre-x_post)**2+(y_pre-y_post)**2+(z_pre-z_post)**2) / (({489.9}*umeter)**2)))')\n",
    "    ratio_L4_L2_L3_e2i = len(testing_S_L4_L2_L3_e2i_dist.N_outgoing_pre) / len(testing_S_L4_L2_L3_e2i_no_dist.N_outgoing_pre)\n",
    "\n",
    "    testing_S_L4_L2_L3_i2e_no_dist = Synapses(G_L4_inh, G_L2_L3_exc, on_pre=\"hi_post+=\"+str(gain)+\"*\"+str(g_max_i/siemens)+\"*siemens\")\n",
    "    testing_S_L4_L2_L3_i2e_dist = Synapses(G_L4_inh, G_L2_L3_exc, on_pre=\"hi_post+=\"+str(gain)+\"*\"+str(g_max_i/siemens)+\"*siemens\")\n",
    "    testing_S_L4_L2_L3_i2e_no_dist.connect(p=current_variables['p'][2])\n",
    "    testing_S_L4_L2_L3_i2e_dist.connect(p=f'{current_variables['p'][2]}*exp(-(((x_pre-x_post)**2+(y_pre-y_post)**2+(z_pre-z_post)**2) / (({489.9}*umeter)**2)))')\n",
    "    ratio_L4_L2_L3_i2e = len(testing_S_L4_L2_L3_i2e_dist.N_outgoing_pre) / len(testing_S_L4_L2_L3_i2e_no_dist.N_outgoing_pre)\n",
    "\n",
    "    testing_S_L4_L2_L3_i2i_no_dist = Synapses(G_L4_inh, G_L2_L3_inh, on_pre=\"hi_post+=\"+str(gain)+\"*\"+str(g_max_i/siemens)+\"*siemens\")\n",
    "    testing_S_L4_L2_L3_i2i_dist = Synapses(G_L4_inh, G_L2_L3_inh, on_pre=\"hi_post+=\"+str(gain)+\"*\"+str(g_max_i/siemens)+\"*siemens\")\n",
    "    testing_S_L4_L2_L3_i2i_no_dist.connect(p=current_variables['p'][3])\n",
    "    testing_S_L4_L2_L3_i2i_dist.connect(p=f'{current_variables['p'][3]}*exp(-(((x_pre-x_post)**2+(y_pre-y_post)**2+(z_pre-z_post)**2) / (({489.9}*umeter)**2)))')\n",
    "    ratio_L4_L2_L3_i2i = len(testing_S_L4_L2_L3_i2i_dist.N_outgoing_pre) / len(testing_S_L4_L2_L3_i2i_no_dist.N_outgoing_pre)\n",
    "\n",
    "    # These ratios can be used to multiply with the given probability in the 'connect()' function to ensure that the same number of connections would have been made if the distance measure was also included in the probability.\n",
    "    S_L4_L2_L3_e2e = Synapses(G_L4_exc, G_L2_L3_exc, on_pre=\"he_post+=\"+str(gain)+\"*\"+str(g_max_e/siemens)+\"*siemens*glu_pre\", name='synapses_between_L4_L2_L3_e2e')\n",
    "    S_L4_L2_L3_e2i = Synapses(G_L4_exc, G_L2_L3_inh, on_pre=\"he_post+=\"+str(gain)+\"*\"+str(g_max_e/siemens)+\"*siemens*glu_pre\", name='synapses_between_L4_L2_L3_e2i')\n",
    "    S_L4_L2_L3_i2e = Synapses(G_L4_inh, G_L2_L3_exc, on_pre=\"hi_post+=\"+str(gain)+\"*\"+str(g_max_i/siemens)+\"*siemens\", name='synapses_between_L4_L2_L3_i2e')\n",
    "    S_L4_L2_L3_i2i = Synapses(G_L4_inh, G_L2_L3_inh, on_pre=\"hi_post+=\"+str(gain)+\"*\"+str(g_max_i/siemens)+\"*siemens\", name='synapses_between_L4_L2_L3_i2i')\n",
    "    S_L4_L2_L3_e2e.connect(p=current_variables['p'][0]*ratio_L4_L2_L3_e2e)\n",
    "    S_L4_L2_L3_e2i.connect(p=current_variables['p'][1]*ratio_L4_L2_L3_e2i)\n",
    "    S_L4_L2_L3_i2e.connect(p=current_variables['p'][2]*ratio_L4_L2_L3_i2e)\n",
    "    S_L4_L2_L3_i2i.connect(p=current_variables['p'][3]*ratio_L4_L2_L3_i2i)\n",
    "    synapses.append(S_L4_L2_L3_e2e)\n",
    "    synapses.append(S_L4_L2_L3_e2i)\n",
    "    synapses.append(S_L4_L2_L3_i2e)\n",
    "    synapses.append(S_L4_L2_L3_i2i)\n",
    "\n",
    "    \n",
    "    # Since we want the total number of connections from the L5 to the L1 layer to be equal when only using the given probability versus when using both the given probability and distance measure, we can examine how many connections are made for each case.\n",
    "    # This is done separately for all four possible combinations of neuron type connections.\n",
    "    testing_S_L5_L1_e2e_no_dist = Synapses(G_L5_exc, G_L1_exc, on_pre=\"he_post+=\"+str(gain)+\"*\"+str(g_max_e/siemens)+\"*siemens*glu_pre\")\n",
    "    testing_S_L5_L1_e2e_dist = Synapses(G_L5_exc, G_L1_exc, on_pre=\"he_post+=\"+str(gain)+\"*\"+str(g_max_e/siemens)+\"*siemens*glu_pre\")\n",
    "    testing_S_L5_L1_e2e_no_dist.connect(p=current_variables['p'][0])\n",
    "    testing_S_L5_L1_e2e_dist.connect(p=f'{current_variables['p'][0]}*exp(-(((x_pre-x_post)**2+(y_pre-y_post)**2+(z_pre-z_post)**2) / (({489.9}*umeter)**2)))')\n",
    "    ratio_L5_L1_e2e = len(testing_S_L5_L1_e2e_dist.N_outgoing_pre) / len(testing_S_L5_L1_e2e_no_dist.N_outgoing_pre)\n",
    "    \n",
    "    testing_S_L5_L1_e2i_no_dist = Synapses(G_L5_exc, G_L1_inh, on_pre=\"he_post+=\"+str(gain)+\"*\"+str(g_max_e/siemens)+\"*siemens*glu_pre\")\n",
    "    testing_S_L5_L1_e2i_dist = Synapses(G_L5_exc, G_L1_inh, on_pre=\"he_post+=\"+str(gain)+\"*\"+str(g_max_e/siemens)+\"*siemens*glu_pre\")\n",
    "    testing_S_L5_L1_e2i_no_dist.connect(p=current_variables['p'][1])\n",
    "    testing_S_L5_L1_e2i_dist.connect(p=f'{current_variables['p'][1]}*exp(-(((x_pre-x_post)**2+(y_pre-y_post)**2+(z_pre-z_post)**2) / (({489.9}*umeter)**2)))')\n",
    "    ratio_L5_L1_e2i = len(testing_S_L5_L1_e2i_dist.N_outgoing_pre) / len(testing_S_L5_L1_e2i_no_dist.N_outgoing_pre)\n",
    "\n",
    "    testing_S_L5_L1_i2e_no_dist = Synapses(G_L5_inh, G_L1_exc, on_pre=\"hi_post+=\"+str(gain)+\"*\"+str(g_max_i/siemens)+\"*siemens\")\n",
    "    testing_S_L5_L1_i2e_dist = Synapses(G_L5_inh, G_L1_exc, on_pre=\"hi_post+=\"+str(gain)+\"*\"+str(g_max_i/siemens)+\"*siemens\")\n",
    "    testing_S_L5_L1_i2e_no_dist.connect(p=current_variables['p'][2])\n",
    "    testing_S_L5_L1_i2e_dist.connect(p=f'{current_variables['p'][2]}*exp(-(((x_pre-x_post)**2+(y_pre-y_post)**2+(z_pre-z_post)**2) / (({489.9}*umeter)**2)))')\n",
    "    ratio_L5_L1_i2e = len(testing_S_L5_L1_i2e_dist.N_outgoing_pre) / len(testing_S_L5_L1_i2e_no_dist.N_outgoing_pre)\n",
    "\n",
    "    testing_S_L5_L1_i2i_no_dist = Synapses(G_L5_inh, G_L1_inh, on_pre=\"hi_post+=\"+str(gain)+\"*\"+str(g_max_i/siemens)+\"*siemens\")\n",
    "    testing_S_L5_L1_i2i_dist = Synapses(G_L5_inh, G_L1_inh, on_pre=\"hi_post+=\"+str(gain)+\"*\"+str(g_max_i/siemens)+\"*siemens\")\n",
    "    testing_S_L5_L1_i2i_no_dist.connect(p=current_variables['p'][3])\n",
    "    testing_S_L5_L1_i2i_dist.connect(p=f'{current_variables['p'][3]}*exp(-(((x_pre-x_post)**2+(y_pre-y_post)**2+(z_pre-z_post)**2) / (({489.9}*umeter)**2)))')\n",
    "    ratio_L5_L1_i2i = len(testing_S_L5_L1_i2i_dist.N_outgoing_pre) / len(testing_S_L5_L1_i2i_no_dist.N_outgoing_pre)\n",
    "\n",
    "    # These ratios can be used to multiply with the given probability in the 'connect()' function to ensure that the same number of connections would have been made if the distance measure was also included in the probability.\n",
    "    S_L5_L1_e2e = Synapses(G_L5_exc, G_L1_exc, on_pre=\"he_post+=\"+str(gain)+\"*\"+str(g_max_e/siemens)+\"*siemens*glu_pre\", name='synapses_between_L5_L1_e2e')\n",
    "    S_L5_L1_e2i = Synapses(G_L5_exc, G_L1_inh, on_pre=\"he_post+=\"+str(gain)+\"*\"+str(g_max_e/siemens)+\"*siemens*glu_pre\", name='synapses_between_L5_L1_e2i')\n",
    "    S_L5_L1_i2e = Synapses(G_L5_inh, G_L1_exc, on_pre=\"hi_post+=\"+str(gain)+\"*\"+str(g_max_i/siemens)+\"*siemens\", name='synapses_between_L5_L1_i2e')\n",
    "    S_L5_L1_i2i = Synapses(G_L5_inh, G_L1_inh, on_pre=\"hi_post+=\"+str(gain)+\"*\"+str(g_max_i/siemens)+\"*siemens\", name='synapses_between_L5_L1_i2i')\n",
    "    S_L5_L1_e2e.connect(p=current_variables['p'][0]*ratio_L5_L1_e2e)\n",
    "    S_L5_L1_e2i.connect(p=current_variables['p'][1]*ratio_L5_L1_e2i)\n",
    "    S_L5_L1_i2e.connect(p=current_variables['p'][2]*ratio_L5_L1_i2e)\n",
    "    S_L5_L1_i2i.connect(p=current_variables['p'][3]*ratio_L5_L1_i2i)\n",
    "    synapses.append(S_L5_L1_e2e)\n",
    "    synapses.append(S_L5_L1_e2i)\n",
    "    synapses.append(S_L5_L1_i2e)\n",
    "    synapses.append(S_L5_L1_i2i)\n",
    "\n",
    "\n",
    "    # Since we want the total number of connections from the L5 to the L2_L3 layer to be equal when only using the given probability versus when using both the given probability and distance measure, we can examine how many connections are made for each case.\n",
    "    # This is done separately for all four possible combinations of neuron type connections.\n",
    "    testing_S_L5_L2_L3_e2e_no_dist = Synapses(G_L5_exc, G_L2_L3_exc, on_pre=\"he_post+=\"+str(gain)+\"*\"+str(g_max_e/siemens)+\"*siemens*glu_pre\")\n",
    "    testing_S_L5_L2_L3_e2e_dist = Synapses(G_L5_exc, G_L2_L3_exc, on_pre=\"he_post+=\"+str(gain)+\"*\"+str(g_max_e/siemens)+\"*siemens*glu_pre\")\n",
    "    testing_S_L5_L2_L3_e2e_no_dist.connect(p=current_variables['p'][0])\n",
    "    testing_S_L5_L2_L3_e2e_dist.connect(p=f'{current_variables['p'][0]}*exp(-(((x_pre-x_post)**2+(y_pre-y_post)**2+(z_pre-z_post)**2) / (({489.9}*umeter)**2)))')\n",
    "    ratio_L5_L2_L3_e2e = len(testing_S_L5_L2_L3_e2e_dist.N_outgoing_pre) / len(testing_S_L5_L2_L3_e2e_no_dist.N_outgoing_pre)\n",
    "    \n",
    "    testing_S_L5_L2_L3_e2i_no_dist = Synapses(G_L5_exc, G_L2_L3_inh, on_pre=\"he_post+=\"+str(gain)+\"*\"+str(g_max_e/siemens)+\"*siemens*glu_pre\")\n",
    "    testing_S_L5_L2_L3_e2i_dist = Synapses(G_L5_exc, G_L2_L3_inh, on_pre=\"he_post+=\"+str(gain)+\"*\"+str(g_max_e/siemens)+\"*siemens*glu_pre\")\n",
    "    testing_S_L5_L2_L3_e2i_no_dist.connect(p=current_variables['p'][1])\n",
    "    testing_S_L5_L2_L3_e2i_dist.connect(p=f'{current_variables['p'][1]}*exp(-(((x_pre-x_post)**2+(y_pre-y_post)**2+(z_pre-z_post)**2) / (({489.9}*umeter)**2)))')\n",
    "    ratio_L5_L2_L3_e2i = len(testing_S_L5_L2_L3_e2i_dist.N_outgoing_pre) / len(testing_S_L5_L2_L3_e2i_no_dist.N_outgoing_pre)\n",
    "\n",
    "    testing_S_L5_L2_L3_i2e_no_dist = Synapses(G_L5_inh, G_L2_L3_exc, on_pre=\"hi_post+=\"+str(gain)+\"*\"+str(g_max_i/siemens)+\"*siemens\")\n",
    "    testing_S_L5_L2_L3_i2e_dist = Synapses(G_L5_inh, G_L2_L3_exc, on_pre=\"hi_post+=\"+str(gain)+\"*\"+str(g_max_i/siemens)+\"*siemens\")\n",
    "    testing_S_L5_L2_L3_i2e_no_dist.connect(p=current_variables['p'][2])\n",
    "    testing_S_L5_L2_L3_i2e_dist.connect(p=f'{current_variables['p'][2]}*exp(-(((x_pre-x_post)**2+(y_pre-y_post)**2+(z_pre-z_post)**2) / (({489.9}*umeter)**2)))')\n",
    "    ratio_L5_L2_L3_i2e = len(testing_S_L5_L2_L3_i2e_dist.N_outgoing_pre) / len(testing_S_L5_L2_L3_i2e_no_dist.N_outgoing_pre)\n",
    "\n",
    "    testing_S_L5_L2_L3_i2i_no_dist = Synapses(G_L5_inh, G_L2_L3_inh, on_pre=\"hi_post+=\"+str(gain)+\"*\"+str(g_max_i/siemens)+\"*siemens\")\n",
    "    testing_S_L5_L2_L3_i2i_dist = Synapses(G_L5_inh, G_L2_L3_inh, on_pre=\"hi_post+=\"+str(gain)+\"*\"+str(g_max_i/siemens)+\"*siemens\")\n",
    "    testing_S_L5_L2_L3_i2i_no_dist.connect(p=current_variables['p'][3])\n",
    "    testing_S_L5_L2_L3_i2i_dist.connect(p=f'{current_variables['p'][3]}*exp(-(((x_pre-x_post)**2+(y_pre-y_post)**2+(z_pre-z_post)**2) / (({489.9}*umeter)**2)))')\n",
    "    ratio_L5_L2_L3_i2i = len(testing_S_L5_L2_L3_i2i_dist.N_outgoing_pre) / len(testing_S_L5_L2_L3_i2i_no_dist.N_outgoing_pre)\n",
    "\n",
    "    # These ratios can be used to multiply with the given probability in the 'connect()' function to ensure that the same number of connections would have been made if the distance measure was also included in the probability.\n",
    "    S_L5_L2_L3_e2e = Synapses(G_L5_exc, G_L2_L3_exc, on_pre=\"he_post+=\"+str(gain)+\"*\"+str(g_max_e/siemens)+\"*siemens*glu_pre\", name='synapses_between_L5_L2_L3_e2e')\n",
    "    S_L5_L2_L3_e2i = Synapses(G_L5_exc, G_L2_L3_inh, on_pre=\"he_post+=\"+str(gain)+\"*\"+str(g_max_e/siemens)+\"*siemens*glu_pre\", name='synapses_between_L5_L2_L3_e2i')\n",
    "    S_L5_L2_L3_i2e = Synapses(G_L5_inh, G_L2_L3_exc, on_pre=\"hi_post+=\"+str(gain)+\"*\"+str(g_max_i/siemens)+\"*siemens\", name='synapses_between_L5_L2_L3_i2e')\n",
    "    S_L5_L2_L3_i2i = Synapses(G_L5_inh, G_L2_L3_inh, on_pre=\"hi_post+=\"+str(gain)+\"*\"+str(g_max_i/siemens)+\"*siemens\", name='synapses_between_L5_L2_L3_i2i')\n",
    "    S_L5_L2_L3_e2e.connect(p=current_variables['p'][0]*ratio_L5_L2_L3_e2e)\n",
    "    S_L5_L2_L3_e2i.connect(p=current_variables['p'][1]*ratio_L5_L2_L3_e2i)\n",
    "    S_L5_L2_L3_i2e.connect(p=current_variables['p'][2]*ratio_L5_L2_L3_i2e)\n",
    "    S_L5_L2_L3_i2i.connect(p=current_variables['p'][3]*ratio_L5_L2_L3_i2i)\n",
    "    synapses.append(S_L5_L2_L3_e2e)\n",
    "    synapses.append(S_L5_L2_L3_e2i)\n",
    "    synapses.append(S_L5_L2_L3_i2e)\n",
    "    synapses.append(S_L5_L2_L3_i2i)\n",
    "\n",
    "\n",
    "    # Since we want the total number of connections from the L6 to the L4 layer to be equal when only using the given probability versus when using both the given probability and distance measure, we can examine how many connections are made for each case.\n",
    "    # This is done separately for all four possible combinations of neuron type connections.\n",
    "    testing_S_L6_L4_e2e_no_dist = Synapses(G_L6_exc, G_L4_exc, on_pre=\"he_post+=\"+str(gain)+\"*\"+str(g_max_e/siemens)+\"*siemens*glu_pre\")\n",
    "    testing_S_L6_L4_e2e_dist = Synapses(G_L6_exc, G_L4_exc, on_pre=\"he_post+=\"+str(gain)+\"*\"+str(g_max_e/siemens)+\"*siemens*glu_pre\")\n",
    "    testing_S_L6_L4_e2e_no_dist.connect(p=current_variables['p'][0])\n",
    "    testing_S_L6_L4_e2e_dist.connect(p=f'{current_variables['p'][0]}*exp(-(((x_pre-x_post)**2+(y_pre-y_post)**2+(z_pre-z_post)**2) / (({489.9}*umeter)**2)))')\n",
    "    ratio_L6_L4_e2e = len(testing_S_L6_L4_e2e_dist.N_outgoing_pre) / len(testing_S_L6_L4_e2e_no_dist.N_outgoing_pre)\n",
    "    \n",
    "    testing_S_L6_L4_e2i_no_dist = Synapses(G_L6_exc, G_L4_inh, on_pre=\"he_post+=\"+str(gain)+\"*\"+str(g_max_e/siemens)+\"*siemens*glu_pre\")\n",
    "    testing_S_L6_L4_e2i_dist = Synapses(G_L6_exc, G_L4_inh, on_pre=\"he_post+=\"+str(gain)+\"*\"+str(g_max_e/siemens)+\"*siemens*glu_pre\")\n",
    "    testing_S_L6_L4_e2i_no_dist.connect(p=current_variables['p'][1])\n",
    "    testing_S_L6_L4_e2i_dist.connect(p=f'{current_variables['p'][1]}*exp(-(((x_pre-x_post)**2+(y_pre-y_post)**2+(z_pre-z_post)**2) / (({489.9}*umeter)**2)))')\n",
    "    ratio_L6_L4_e2i = len(testing_S_L6_L4_e2i_dist.N_outgoing_pre) / len(testing_S_L6_L4_e2i_no_dist.N_outgoing_pre)\n",
    "\n",
    "    testing_S_L6_L4_i2e_no_dist = Synapses(G_L6_inh, G_L4_exc, on_pre=\"hi_post+=\"+str(gain)+\"*\"+str(g_max_i/siemens)+\"*siemens\")\n",
    "    testing_S_L6_L4_i2e_dist = Synapses(G_L6_inh, G_L4_exc, on_pre=\"hi_post+=\"+str(gain)+\"*\"+str(g_max_i/siemens)+\"*siemens\")\n",
    "    testing_S_L6_L4_i2e_no_dist.connect(p=current_variables['p'][2])\n",
    "    testing_S_L6_L4_i2e_dist.connect(p=f'{current_variables['p'][2]}*exp(-(((x_pre-x_post)**2+(y_pre-y_post)**2+(z_pre-z_post)**2) / (({489.9}*umeter)**2)))')\n",
    "    ratio_L6_L4_i2e = len(testing_S_L6_L4_i2e_dist.N_outgoing_pre) / len(testing_S_L6_L4_i2e_no_dist.N_outgoing_pre)\n",
    "\n",
    "    testing_S_L6_L4_i2i_no_dist = Synapses(G_L6_inh, G_L4_inh, on_pre=\"hi_post+=\"+str(gain)+\"*\"+str(g_max_i/siemens)+\"*siemens\")\n",
    "    testing_S_L6_L4_i2i_dist = Synapses(G_L6_inh, G_L4_inh, on_pre=\"hi_post+=\"+str(gain)+\"*\"+str(g_max_i/siemens)+\"*siemens\")\n",
    "    testing_S_L6_L4_i2i_no_dist.connect(p=current_variables['p'][3])\n",
    "    testing_S_L6_L4_i2i_dist.connect(p=f'{current_variables['p'][3]}*exp(-(((x_pre-x_post)**2+(y_pre-y_post)**2+(z_pre-z_post)**2) / (({489.9}*umeter)**2)))')\n",
    "    ratio_L6_L4_i2i = len(testing_S_L6_L4_i2i_dist.N_outgoing_pre) / len(testing_S_L6_L4_i2i_no_dist.N_outgoing_pre)\n",
    "\n",
    "    # These ratios can be used to multiply with the given probability in the 'connect()' function to ensure that the same number of connections would have been made if the distance measure was also included in the probability.\n",
    "    S_L6_L4_e2e = Synapses(G_L6_exc, G_L4_exc, on_pre=\"he_post+=\"+str(gain)+\"*\"+str(g_max_e/siemens)+\"*siemens*glu_pre\", name='synapses_between_L6_L4_e2e')\n",
    "    S_L6_L4_e2i = Synapses(G_L6_exc, G_L4_inh, on_pre=\"he_post+=\"+str(gain)+\"*\"+str(g_max_e/siemens)+\"*siemens*glu_pre\", name='synapses_between_L6_L4_e2i')\n",
    "    S_L6_L4_i2e = Synapses(G_L6_inh, G_L4_exc, on_pre=\"hi_post+=\"+str(gain)+\"*\"+str(g_max_i/siemens)+\"*siemens\", name='synapses_between_L6_L4_i2e')\n",
    "    S_L6_L4_i2i = Synapses(G_L6_inh, G_L4_inh, on_pre=\"hi_post+=\"+str(gain)+\"*\"+str(g_max_i/siemens)+\"*siemens\", name='synapses_between_L6_L4_i2i')\n",
    "    S_L6_L4_e2e.connect(p=current_variables['p'][0]*ratio_L6_L4_e2e)\n",
    "    S_L6_L4_e2i.connect(p=current_variables['p'][1]*ratio_L6_L4_e2i)\n",
    "    S_L6_L4_i2e.connect(p=current_variables['p'][2]*ratio_L6_L4_i2e)\n",
    "    S_L6_L4_i2i.connect(p=current_variables['p'][3]*ratio_L6_L4_i2i)\n",
    "    synapses.append(S_L6_L4_e2e)\n",
    "    synapses.append(S_L6_L4_e2i)\n",
    "    synapses.append(S_L6_L4_i2e)\n",
    "    synapses.append(S_L6_L4_i2i)\n",
    "\n",
    "\n",
    "    \n",
    "    ########################################################      \n",
    "    ## Connections Between Electrode and Electrode layer ##\n",
    "    ########################################################\n",
    "    # Configuring synaptic connections between the excitatory neurons and the LFP electrode based on what the layer of the LFP electrode is.\n",
    "    if current_variables['LFP_electrode_layer'] == \"L1\":\n",
    "        S_lfp = Synapses(G_L1_exc, G_lfp, model='''w : ohm*meter**2 (constant) # Weight in the LFP calculation\n",
    "                                               v_post = w*((0.0*amp/meter**2)-Im_pre) : volt (summed)''')\n",
    "    if current_variables['LFP_electrode_layer'] == \"L2_L3\":\n",
    "        S_lfp = Synapses(G_L2_L3_exc, G_lfp, model='''w : ohm*meter**2 (constant) # Weight in the LFP calculation\n",
    "                                       v_post = w*((0.0*amp/meter**2)-Im_pre) : volt (summed)''')\n",
    "    if current_variables['LFP_electrode_layer'] == \"L4\":\n",
    "        S_lfp = Synapses(G_L4_exc, G_lfp, model='''w : ohm*meter**2 (constant) # Weight in the LFP calculation\n",
    "                                       v_post = w*((0.0*amp/meter**2)-Im_pre) : volt (summed)''')\n",
    "    if current_variables['LFP_electrode_layer'] == \"L5\":\n",
    "        S_lfp = Synapses(G_L5_exc, G_lfp, model='''w : ohm*meter**2 (constant) # Weight in the LFP calculation\n",
    "                                       v_post = w*((0.0*amp/meter**2)-Im_pre) : volt (summed)''')\n",
    "    if current_variables['LFP_electrode_layer'] == \"L6\":\n",
    "        S_lfp = Synapses(G_L6_exc, G_lfp, model='''w : ohm*meter**2 (constant) # Weight in the LFP calculation\n",
    "                                       v_post = w*((0.0*amp/meter**2)-Im_pre) : volt (summed)''')\n",
    "\n",
    "    # Ensuring LFP voltage is updated after neuron groups.\n",
    "    S_lfp.summed_updaters['v_post'].when = 'after_groups' \n",
    "\n",
    "    # Generating the connections between the excitatory neurons and the LFP electrode. Here, all of them will be connected since there is no probability defined.\n",
    "    S_lfp.connect()\n",
    "\n",
    "    # Setting the weight for LFP calculation which is scaled by the Euclidean distance measure.\n",
    "    S_lfp.w = '(29e3 * umetre ** 2)/(4*pi*sigma)/((x_pre-x_post)**2+(y_pre-y_post)**2+(z_pre-z_post)**2)**.5'\n",
    "\n",
    "    synapses.append(S_lfp)\n",
    "\n",
    "\n",
    "    ############################################\n",
    "    ########  Final Network Definition  ########\n",
    "    ############################################\n",
    "    net = Network(neuron_groups, synapses, monitors)\n",
    "\n",
    "    \n",
    "    return net, synapses, monitors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62a2d875-6c5a-4dfc-9164-127b36ac8de5",
   "metadata": {},
   "source": [
    "<br></br><br></br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "948b1d3c-cddb-48f2-bd18-558925b4ba68",
   "metadata": {},
   "source": [
    "<h2 style=\"font-size: 40px;\">Simulation Functions</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9eb10c34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Runs the simulation of the neural network in discrete time fragments which includes a mechanism to alter neuron parameters based on a firing rate threshold, which simulates a treatment effect.\n",
    "def run_granular_simulation(net, variables, treatment_settings, monitors):\n",
    "    \n",
    "    print('#######################')\n",
    "    print('# Starting Simulation #')\n",
    "    print('#######################')\n",
    "    print()\n",
    "\n",
    "    # Extracting the passed on parameters.\n",
    "    total_duration = variables['duration']\n",
    "    input_signal = read_input_signal(variables['input_signal_file'])\n",
    "    time_fragment, firing_rate_threshold = treatment_settings\n",
    "    popmon_L1_exc, popmon_L2_L3_exc, popmon_L4_exc, popmon_L5_exc, popmon_L6_exc, popmon_L1_inh, popmon_L2_L3_inh, popmon_L4_inh, popmon_L5_inh, popmon_L6_inh, Mlfp, statemon_L1_exc, statemon_L2_L3_exc, statemon_L4_exc, statemon_L5_exc, statemon_L6_exc, statemon_L1_inh, statemon_L2_L3_inh, statemon_L4_inh, statemon_L5_inh, statemon_L6_inh = monitors\n",
    "    \n",
    "    # Setting the potassium equilibrium potentials for both the excitatory and inhibitory neurons.\n",
    "    Eke = variables['Eke_baseline']\n",
    "    Eki = variables['Eki_baseline']\n",
    "    Eke_baseline = variables['Eke_baseline']\n",
    "    Eki_baseline = variables['Eki_baseline']\n",
    "    \n",
    "    print('Treatment Parameters:', 'time sensitivity', time_fragment, 'FR Threshold:', firing_rate_threshold)\n",
    "    print()\n",
    "\n",
    "    # Converting the time fragments to milliseconds and determining the number of batches.\n",
    "    time_fragment_ms = int(time_fragment/ms)\n",
    "    num_batches = int(total_duration / time_fragment)\n",
    "\n",
    "    # For every batch, we run the simulation on the network 'net'. Here the 'tqdm()' function is used which creates a progress bar.\n",
    "    for i in tqdm(range(num_batches), desc=\"Running Simulation\"): \n",
    "        net.run(time_fragment)\n",
    "\n",
    "        # If after running a time fragment, the average firing rate of the excitatory neurons over the last time fragment exceeds the threshold, treatment is initiated.\n",
    "        # This statement refers to the scenario where epilepsy is detected and treated by adjusting the potassium equilibrium potentials of the excitatory and inhibitory neurons.\n",
    "        if np.mean(popmon_L4_exc.rate[-time_fragment_ms:]) > firing_rate_threshold:\n",
    "            Eke = variables['Eke_treatment']\n",
    "            Eki = variables['Eki_treatment']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfab6714-6985-4982-8954-75ed1c8a8eae",
   "metadata": {},
   "source": [
    "<br></br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e34f6cc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function manages the overall process of setting up and running multiple instances of a neural network simulation as after setting everything up, the function 'run_granular_simulation()' is called.\n",
    "# In addition, it also handles the storing of the results.\n",
    "def run_model_loop(variables):\n",
    "\n",
    "    # Checking whether lengths of each variable list is equal.\n",
    "    if not check_dict_lenghts(variables):\n",
    "        raise ValueError('Lenghts of each variable list has be to equal!')\n",
    "    \n",
    "    # For every run provided in the 'run_id' field of the variables dictionary we perform the simulation (which requires some setting up first).\n",
    "    for i in range(len(variables['run_id'])):\n",
    "        \n",
    "        ##########################################\n",
    "        ########  Basic Simulation Setup  ########\n",
    "        ##########################################\n",
    "        # Resetting the state of the simulation environment (to avoid interference from the previous run) and seeting the default simulation time step.\n",
    "        start_scope()\n",
    "        defaultclock.dt = 0.001*second   \n",
    "\n",
    "        # Extracting the list of variables required for the current run.\n",
    "        current_variables = {key: variables[key][i] for key in variables}\n",
    "        print(current_variables['noise_exc'])\n",
    "\n",
    "        # Retrieving the treatment settings.\n",
    "        treatment_settings = [current_variables['device_sensitivity'], current_variables['firing_rate_threshold']]\n",
    "\n",
    "        # Creating a folder to store the results.\n",
    "        run_id = current_variables['run_id']\n",
    "        os.mkdir(f'./results/{run_id}')\n",
    "        write_run_settings(current_variables, run_id)\n",
    "\n",
    "        \n",
    "        #######################################\n",
    "        ########  Creating Topologies  ########\n",
    "        #######################################\n",
    "        # Creating the topologies of both the excitatory and inhibitory neurons.\n",
    "        layer_data, layer_geometrics, excitatory_positions, inhibitory_positions = create_complete_neuron_topology_cortex(current_variables['N'], current_variables['layer_densities'], current_variables['layer_volumes'], current_variables['excitatory_ratios'], current_variables['layer_names'])\n",
    "        \n",
    "        # Assigning the topologies of the excitatory neurons of the different layers to separate variables.\n",
    "        topology_L1_exc = excitatory_positions['L1']\n",
    "        topology_L2_L3_exc = excitatory_positions['L2_L3']\n",
    "        topology_L4_exc = excitatory_positions['L4']\n",
    "        topology_L5_exc = excitatory_positions['L5']\n",
    "        topology_L6_exc = excitatory_positions['L6']\n",
    "\n",
    "        # Assigning the topologies of the inhibitory neurons of the different layers to separate variables.\n",
    "        topology_L1_inh = inhibitory_positions['L1']\n",
    "        topology_L2_L3_inh = inhibitory_positions['L2_L3']\n",
    "        topology_L4_inh = inhibitory_positions['L4']\n",
    "        topology_L5_inh = inhibitory_positions['L5']\n",
    "        topology_L6_inh = inhibitory_positions['L6'] \n",
    "\n",
    "        # Calculating the lengths of the excitatory and inhibitory topologies.\n",
    "        excitatory_topologies_lengths = [len(topology_L1_exc[0]), len(topology_L2_L3_exc[0]), len(topology_L4_exc[0]), len(topology_L5_exc[0]), len(topology_L6_exc[0])]\n",
    "        inhibitory_topologies_lengths = [len(topology_L1_inh[0]), len(topology_L2_L3_inh[0]), len(topology_L4_inh[0]), len(topology_L5_inh[0]), len(topology_L6_inh[0])]\n",
    "\n",
    "        # Adding all the topologies of the neurons to a list.\n",
    "        topologies = [topology_L1_exc, topology_L2_L3_exc, topology_L4_exc, topology_L5_exc, topology_L6_exc, topology_L1_inh, topology_L2_L3_inh, topology_L4_inh, topology_L5_inh, topology_L6_inh] \n",
    "        topology_names = ['L1_exc', 'L2_L3_exc', 'L4_exc', 'L5_exc', 'L6_exc', 'L1_inh', 'L2_L3_inh', 'L4_inh', 'L5_inh', 'L6_inh']\n",
    "\n",
    "        # Updating the 'current_variables' by adding the 'layer_data' and 'layer_geometrics'.\n",
    "        current_variables['layer_data'] = layer_data\n",
    "        current_variables['layer_geometrics'] = layer_geometrics\n",
    "\n",
    "        # Plotting all the cortex layers/topologies in a single 3D plot.\n",
    "        plot_all_cortex_topologies(topologies, topology_names, run_id)\n",
    "\n",
    "\n",
    "        ############################################\n",
    "        ########  Setting up Stimulus Mask  ########\n",
    "        ############################################\n",
    "        # Defining which topologies should be considered for the creation of the stimulus mask according to the stimulus layers.\n",
    "        topology_exc_stimulus = [[],[],[]]\n",
    "        for stimulus_layer in current_variables['stimulus_layers']:\n",
    "            for i, (sublist1, sublist2) in enumerate(zip_longest(topology_exc_stimulus, excitatory_positions[stimulus_layer], fillvalue=[])):\n",
    "                topology_exc_stimulus[i].extend(sublist2)\n",
    "\n",
    "        # If the shape of the stimulus mask is set to 'all', then there is no need to check the mask first.\n",
    "        if current_variables['shape_stimulus_mask'] == \"all\":\n",
    "            \n",
    "            ##########################################\n",
    "            ########  Creating Stimulus Mask  ########\n",
    "            ##########################################\n",
    "            stimulus_mask_exc = create_all_mask(topology_exc_stimulus)\n",
    "        \n",
    "        elif current_variables['shape_stimulus_mask'] == \"perc_of_all\":\n",
    "\n",
    "            ##########################################\n",
    "            ########  Creating Stimulus Mask  ########\n",
    "            ##########################################\n",
    "            stimulus_mask_exc = create_prec_of_all_mask(topology_exc_stimulus, current_variables['stimulus_mask_all_perc'])\n",
    "            \n",
    "        else:\n",
    "            # Checking whether the stimulus mask with the provided coordinate center and radius can be applied to the list of layers by calling the function 'checking_mask()'.\n",
    "            coord_of_stimulus, stimulus_radius_or_edge_length = checking_mask(current_variables['shape_stimulus_mask'], current_variables['stimulus_layers'], current_variables['stimulus_center_coordinates'], current_variables['stimulus_radius_or_edge_length'], current_variables['layer_names'], layer_geometrics)\n",
    "            \n",
    "            # Setting the overall geometry settings of the stimulus mask which includes both the middle point as well as the radius.\n",
    "            stimulus_geometry_settings = [coord_of_stimulus, stimulus_radius_or_edge_length]\n",
    "            \n",
    "            ##########################################\n",
    "            ########  Creating Stimulus Mask  ########\n",
    "            ##########################################\n",
    "            # Creating the stimulus mask for the defined 'topology_exc_stimulus' with as shape the 'current_variables['shape_stimulus_mask']'.\n",
    "            if current_variables['shape_stimulus_mask'] == \"spherical\":\n",
    "                stimulus_mask_exc = create_spherical_mask(topology_exc_stimulus, stimulus_geometry_settings)\n",
    "            elif current_variables['shape_stimulus_mask'] == \"cubical\":\n",
    "                stimulus_mask_exc = create_cubical_mask(topology_exc_stimulus, stimulus_geometry_settings)\n",
    "\n",
    "\n",
    "        #############################################\n",
    "        ########  Setting up Treatment Mask  ########\n",
    "        #############################################\n",
    "        # Defining which topologies should be considered for the creation of the treatment mask according to the treatment layers.\n",
    "        topology_exc_treatment = [[],[],[]]\n",
    "        topology_inh_treatment = [[],[],[]]\n",
    "        for treatment_layer in current_variables['treatment_layers']:\n",
    "            for i, (sublist1, sublist2) in enumerate(zip_longest(topology_exc_treatment, excitatory_positions[treatment_layer], fillvalue=[])):\n",
    "                topology_exc_treatment[i].extend(sublist2)\n",
    "            for i, (sublist1, sublist2) in enumerate(zip_longest(topology_inh_treatment, inhibitory_positions[treatment_layer], fillvalue=[])):\n",
    "                topology_inh_treatment[i].extend(sublist2)\n",
    "\n",
    "        # If the shape of the treatment mask is set to 'all', then there is no need to check the mask first.\n",
    "        if current_variables['shape_treatment_mask'] == \"all\":\n",
    "\n",
    "            ###########################################\n",
    "            ########  Creating Treatment Mask  ########\n",
    "            ###########################################\n",
    "            treatment_mask_exc = create_all_mask(topology_exc_treatment)\n",
    "            treatment_mask_inh = create_all_mask(topology_inh_treatment)\n",
    "\n",
    "        elif current_variables['shape_treatment_mask'] == \"perc_of_all\":\n",
    "\n",
    "            ###########################################\n",
    "            ########  Creating Treatment Mask  ########\n",
    "            ###########################################\n",
    "            treatment_mask_exc = create_prec_of_all_mask(topology_exc_treatment, current_variables['treatment_mask_all_perc'])\n",
    "            treatment_mask_inh = create_prec_of_all_mask(topology_inh_treatment, current_variables['treatment_mask_all_perc'])\n",
    "            \n",
    "        else:\n",
    "            # Checking whether the treatment mask with the provided coordinate center and radius can be applied to the list of layers by calling the function 'checking_treatment_mask()'.\n",
    "            coord_of_treatment, treatment_radius_or_edge_length = checking_mask(current_variables['shape_treatment_mask'], current_variables['treatment_layers'], current_variables['treatment_center_coordinates'], current_variables['treatment_radius_or_edge_length'], current_variables['layer_names'], layer_geometrics)\n",
    "            \n",
    "            # Setting the overall geometry settings of the treatment mask which includes both the middle point as well as the radius.\n",
    "            treatment_geometry_settings = [coord_of_treatment, treatment_radius_or_edge_length]\n",
    "            \n",
    "            ###########################################\n",
    "            ########  Creating Treatment Mask  ########\n",
    "            ###########################################\n",
    "            # Creating the treatment mask for the defined 'topology_exc_treatment' with as shape the 'current_variables['shape_treatment_mask']'.\n",
    "            if current_variables['shape_treatment_mask'] == \"spherical\":\n",
    "                treatment_mask_exc = create_spherical_mask(topology_exc_treatment, treatment_geometry_settings)\n",
    "                treatment_mask_inh = create_spherical_mask(topology_inh_treatment, treatment_geometry_settings)\n",
    "            elif current_variables['shape_treatment_mask'] == \"cubical\":\n",
    "                treatment_mask_exc = create_cubical_mask(topology_exc_treatment, treatment_geometry_settings)\n",
    "                treatment_mask_inh = create_cubical_mask(topology_inh_treatment, treatment_geometry_settings)\n",
    "\n",
    "        treatment_masks = [treatment_mask_exc, treatment_mask_inh]\n",
    "\n",
    "        # Plotting the stimulus and treatment masks in a 3D plot which also features all hippocampal topologies.\n",
    "        plotting_masks_cortex(topologies, topology_names, [current_variables['topology_names_stimulus'], current_variables['topology_names_treatment']], [stimulus_mask_exc, treatment_masks], excitatory_topologies_lengths, inhibitory_topologies_lengths, run_id)\n",
    "\n",
    "    \n",
    "    \n",
    "        ########################################\n",
    "        ########  Initializing Network  ########\n",
    "        ########################################\n",
    "        # Instantiating the network and setting up the monitors.\n",
    "        net, synapses, monitors = prepare_network(topologies, stimulus_mask_exc, treatment_masks, current_variables)\n",
    "        popmon_L1_exc, popmon_L2_L3_exc, popmon_L4_exc, popmon_L5_exc, popmon_L6_exc, popmon_L1_inh, popmon_L2_L3_inh, popmon_L4_inh, popmon_L5_inh, popmon_L6_inh, Mlfp, statemon_L1_exc, statemon_L2_L3_exc, statemon_L4_exc, statemon_L5_exc, statemon_L6_exc, statemon_L1_inh, statemon_L2_L3_inh, statemon_L4_inh, statemon_L5_inh, statemon_L6_inh = monitors\n",
    "\n",
    "        # Running the simulation with the dynamic objects created above.\n",
    "        run_granular_simulation(net, current_variables, treatment_settings, monitors)\n",
    "\n",
    "\n",
    "        ###########################################\n",
    "        ########  Saving Firing Rate Data  ########\n",
    "        ###########################################\n",
    "        exc_color = 'blue'  # Color for excitatory neurons\n",
    "        inh_color = 'orange'   # Color for inhibitory neurons\n",
    "\n",
    "        # Saving the firing rate data for every single layer.\n",
    "        np.savetxt(f'./results/{run_id}/firing_rate_L1_exc.txt', popmon_L1_exc.rate)\n",
    "        np.savetxt(f'./results/{run_id}/firing_rate_L2_L3_exc.txt', popmon_L2_L3_exc.rate)\n",
    "        np.savetxt(f'./results/{run_id}/firing_rate_L4_exc.txt', popmon_L4_exc.rate)\n",
    "        np.savetxt(f'./results/{run_id}/firing_rate_L5_exc.txt', popmon_L5_exc.rate)\n",
    "        np.savetxt(f'./results/{run_id}/firing_rate_L6_exc.txt', popmon_L6_exc.rate)\n",
    "        np.savetxt(f'./results/{run_id}/firing_rate_L1_inh.txt', popmon_L1_inh.rate)\n",
    "        np.savetxt(f'./results/{run_id}/firing_rate_L2_L3_inh.txt', popmon_L2_L3_inh.rate)\n",
    "        np.savetxt(f'./results/{run_id}/firing_rate_L4_inh.txt', popmon_L4_inh.rate)\n",
    "        np.savetxt(f'./results/{run_id}/firing_rate_L5_inh.txt', popmon_L5_inh.rate)\n",
    "        np.savetxt(f'./results/{run_id}/firing_rate_L6_inh.txt', popmon_L6_inh.rate)\n",
    "        \n",
    "\n",
    "        # Plotting the firing rate data for both the excitatory and inhibitory neurons of the L1 layer.\n",
    "        plt.plot(popmon_L1_exc.t, popmon_L1_exc.rate, label='L1_exc', color=exc_color)\n",
    "        plt.plot(popmon_L1_inh.t, popmon_L1_inh.rate, label='L1_inh', color=inh_color)\n",
    "        plt.legend()\n",
    "        plt.savefig(f'./results/{run_id}/firing_rate_L1_both.png', bbox_inches='tight')\n",
    "        plt.close()\n",
    "        plt.plot(popmon_L1_exc.t, popmon_L1_exc.rate, label='L1_exc', color=exc_color)\n",
    "        plt.legend()\n",
    "        plt.savefig(f'./results/{run_id}/firing_rate_L1_exc.png', bbox_inches='tight')\n",
    "        plt.close()\n",
    "        plt.plot(popmon_L1_inh.t, popmon_L1_inh.rate, label='L1_inh', color=inh_color)\n",
    "        plt.legend()\n",
    "        plt.savefig(f'./results/{run_id}/firing_rate_L1_inh.png', bbox_inches='tight')\n",
    "        plt.close()\n",
    "\n",
    "        # Plotting the firing rate data for both the excitatory and inhibitory neurons of the L2_L3 layer.\n",
    "        plt.plot(popmon_L2_L3_exc.t, popmon_L2_L3_exc.rate, label='L2_L3_exc', color=exc_color)\n",
    "        plt.plot(popmon_L2_L3_inh.t, popmon_L2_L3_inh.rate, label='L2_L3_inh', color=inh_color)\n",
    "        plt.legend()\n",
    "        plt.savefig(f'./results/{run_id}/firing_rate_L2_L3_both.png', bbox_inches='tight')\n",
    "        plt.close()\n",
    "        plt.plot(popmon_L2_L3_exc.t, popmon_L2_L3_exc.rate, label='L2_L3_exc', color=exc_color)\n",
    "        plt.legend()\n",
    "        plt.savefig(f'./results/{run_id}/firing_rate_L2_L3_exc.png', bbox_inches='tight')\n",
    "        plt.close()\n",
    "        plt.plot(popmon_L2_L3_inh.t, popmon_L2_L3_inh.rate, label='L2_L3_inh', color=inh_color)\n",
    "        plt.legend()\n",
    "        plt.savefig(f'./results/{run_id}/firing_rate_L2_L3_inh.png', bbox_inches='tight')\n",
    "        plt.close()\n",
    "\n",
    "        # Plotting the firing rate data for both the excitatory and inhibitory neurons of the L4 layer.\n",
    "        plt.plot(popmon_L4_exc.t, popmon_L4_exc.rate, label='L4_exc', color=exc_color)\n",
    "        plt.plot(popmon_L4_inh.t, popmon_L4_inh.rate, label='L4_inh', color=inh_color)\n",
    "        plt.legend()\n",
    "        plt.savefig(f'./results/{run_id}/firing_rate_L4_both.png', bbox_inches='tight')\n",
    "        plt.close()\n",
    "        plt.plot(popmon_L4_exc.t, popmon_L4_exc.rate, label='L4_exc', color=exc_color)\n",
    "        plt.legend()\n",
    "        plt.savefig(f'./results/{run_id}/firing_rate_L4_exc.png', bbox_inches='tight')\n",
    "        plt.close()\n",
    "        plt.plot(popmon_L4_inh.t, popmon_L4_inh.rate, label='L4_inh', color=inh_color)\n",
    "        plt.legend()\n",
    "        plt.savefig(f'./results/{run_id}/firing_rate_L4_inh.png', bbox_inches='tight')\n",
    "        plt.close()\n",
    "\n",
    "        # Plotting the firing rate data for both the excitatory and inhibitory neurons of the L5 layer.\n",
    "        plt.plot(popmon_L5_exc.t, popmon_L5_exc.rate, label='L5_exc', color=exc_color)\n",
    "        plt.plot(popmon_L5_inh.t, popmon_L5_inh.rate, label='L5_inh', color=inh_color)\n",
    "        plt.legend()\n",
    "        plt.savefig(f'./results/{run_id}/firing_rate_L5_both.png', bbox_inches='tight')\n",
    "        plt.close()\n",
    "        plt.plot(popmon_L5_exc.t, popmon_L5_exc.rate, label='L5_exc', color=exc_color)\n",
    "        plt.legend()\n",
    "        plt.savefig(f'./results/{run_id}/firing_rate_L5_exc.png', bbox_inches='tight')\n",
    "        plt.close()\n",
    "        plt.plot(popmon_L5_inh.t, popmon_L5_inh.rate, label='L5_inh', color=inh_color)\n",
    "        plt.legend()\n",
    "        plt.savefig(f'./results/{run_id}/firing_rate_L5_inh.png', bbox_inches='tight')\n",
    "        plt.close()\n",
    "\n",
    "        # Plotting the firing rate data for both the excitatory and inhibitory neurons of the L6 layer.\n",
    "        plt.plot(popmon_L6_exc.t, popmon_L6_exc.rate, label='L6_exc', color=exc_color)\n",
    "        plt.plot(popmon_L6_inh.t, popmon_L6_inh.rate, label='L6_inh', color=inh_color)\n",
    "        plt.legend()\n",
    "        plt.savefig(f'./results/{run_id}/firing_rate_L6_both.png', bbox_inches='tight')\n",
    "        plt.close()\n",
    "        plt.plot(popmon_L6_exc.t, popmon_L6_exc.rate, label='L6_exc', color=exc_color)\n",
    "        plt.legend()\n",
    "        plt.savefig(f'./results/{run_id}/firing_rate_L6_exc.png', bbox_inches='tight')\n",
    "        plt.close()\n",
    "        plt.plot(popmon_L6_inh.t, popmon_L6_inh.rate, label='L6_inh', color=inh_color)\n",
    "        plt.legend()\n",
    "        plt.savefig(f'./results/{run_id}/firing_rate_L6_inh.png', bbox_inches='tight')\n",
    "        plt.close()\n",
    "\n",
    "\n",
    "        #####################################\n",
    "        ########  Saving Noise Data  ########\n",
    "        #####################################\n",
    "        # Plotting and saving the noise for the excitatory and inhibitory neurons of the L1 layer.\n",
    "        plt.plot(statemon_L1_exc.t, statemon_L1_exc.I_noise[4]/nA, label='exc')\n",
    "        plt.savefig(f'./results/{run_id}/noise_L1_exc.png', bbox_inches='tight')\n",
    "        plt.close()\n",
    "        plt.plot(statemon_L1_inh.t, statemon_L1_inh.I_noise[4]/nA, label='inh')\n",
    "        plt.savefig(f'./results/{run_id}/noise_L1_inh.png', bbox_inches='tight')\n",
    "        plt.close()\n",
    "\n",
    "        # Plotting and saving the noise for the excitatory and inhibitory neurons of the L2_L3 layer.\n",
    "        plt.plot(statemon_L2_L3_exc.t, statemon_L2_L3_exc.I_noise[4]/nA, label='exc')\n",
    "        plt.savefig(f'./results/{run_id}/noise_L2_L3_exc.png', bbox_inches='tight')\n",
    "        plt.close()\n",
    "        plt.plot(statemon_L2_L3_inh.t, statemon_L2_L3_inh.I_noise[4]/nA, label='inh')\n",
    "        plt.savefig(f'./results/{run_id}/noise_L2_L3_inh.png', bbox_inches='tight')\n",
    "        plt.close()\n",
    "\n",
    "        # Plotting and saving the noise for the excitatory and inhibitory neurons of the L4 layer.\n",
    "        plt.plot(statemon_L4_exc.t, statemon_L4_exc.I_noise[4]/nA, label='exc')\n",
    "        plt.savefig(f'./results/{run_id}/noise_L4_exc.png', bbox_inches='tight')\n",
    "        plt.close()\n",
    "        plt.plot(statemon_L4_inh.t, statemon_L4_inh.I_noise[4]/nA, label='inh')\n",
    "        plt.savefig(f'./results/{run_id}/noise_L4_inh.png', bbox_inches='tight')\n",
    "        plt.close()\n",
    "\n",
    "        # Plotting and saving the noise for the excitatory and inhibitory neurons of the L5 layer.\n",
    "        plt.plot(statemon_L5_exc.t, statemon_L5_exc.I_noise[4]/nA, label='exc')\n",
    "        plt.savefig(f'./results/{run_id}/noise_L5_exc.png', bbox_inches='tight')\n",
    "        plt.close()\n",
    "        plt.plot(statemon_L5_inh.t, statemon_L5_inh.I_noise[4]/nA, label='inh')\n",
    "        plt.savefig(f'./results/{run_id}/noise_L5_inh.png', bbox_inches='tight')\n",
    "        plt.close()\n",
    "\n",
    "        # Plotting and saving the noise for the excitatory and inhibitory neurons of the L6 layer.\n",
    "        plt.plot(statemon_L6_exc.t, statemon_L6_exc.I_noise[4]/nA, label='exc')\n",
    "        plt.savefig(f'./results/{run_id}/noise_L6_exc.png', bbox_inches='tight')\n",
    "        plt.close()\n",
    "        plt.plot(statemon_L6_inh.t, statemon_L6_inh.I_noise[4]/nA, label='inh')\n",
    "        plt.savefig(f'./results/{run_id}/noise_L6_inh.png', bbox_inches='tight')\n",
    "        plt.close()\n",
    "\n",
    "\n",
    "        ###################################\n",
    "        ########  Saving LFP Data  ########\n",
    "        ###################################\n",
    "        # Plotting and saving the recorded LFP voltage values in millivolts from the 'Mlfp' monitor. \n",
    "        np.savetxt(f'./results/{run_id}/voltage_LFP.txt', Mlfp.v[0]/mV)\n",
    "        plot(Mlfp.t/ms, Mlfp.v[0]/mV)\n",
    "        plt.savefig(f'./results/{run_id}/voltage_LFP.png', bbox_inches='tight')\n",
    "        plt.close()\n",
    "\n",
    "        \n",
    "        time.sleep(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88a460e7-2078-40bf-950a-0fc9a09e0d10",
   "metadata": {},
   "source": [
    "<br></br><br></br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4172ae1d-5156-47c1-90d5-9711bb7d9e96",
   "metadata": {},
   "source": [
    "<h2 style=\"font-size: 40px;\">Simulation Variables</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7c3fd703-e6b4-40ac-805d-ab2cc4bd5f44",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################\n",
    "########  Simulation Variables  ########\n",
    "########################################\n",
    "\n",
    "# Setting the number of times the simulation will be performed.\n",
    "copy_times = 1\n",
    "\n",
    "# Defining the volumes, neuron densities, and excitatory to inhibitory ratios of the layers.\n",
    "layer_volumes = {\"L1\": 0.12, \n",
    "                 \"L2_L3\": 0.21, \n",
    "                 \"L4\": 0.10, \n",
    "                 \"L5\": 0.26, \n",
    "                 \"L6\": 0.31}\n",
    "L2_neuron_density = 95000\n",
    "L3_neuron_density = 140000\n",
    "L2_volume = 0.07\n",
    "L3_volume = 0.14\n",
    "L2_proportion_L2_L3 = L2_volume / (L2_volume + L3_volume)\n",
    "L3_proportion_L2_L3 = L3_volume / (L2_volume + L3_volume)\n",
    "layer_densities_dict = {\"L1\": 70000, \n",
    "                        \"L2_L3\": L2_proportion_L2_L3 * L2_neuron_density + L3_proportion_L2_L3 * L3_neuron_density,\n",
    "                        \"L4\": 190000, \n",
    "                        \"L5\": 120000, \n",
    "                        \"L6\": 100000}\n",
    "layer_excitatory_ratios_dict = {\"L1\": 0.8, \n",
    "                                \"L2_L3\": 0.8,\n",
    "                                \"L4\": 0.8, \n",
    "                                \"L5\": 0.8, \n",
    "                                \"L6\": 0.8}\n",
    "                            \n",
    "# Defining the vocabulary of variables.\n",
    "variables = {\n",
    "\n",
    "    # Defining basic run variables.\n",
    "    \"run_id\": ['Results 1'],\n",
    "    \"duration\": [4000*ms]*copy_times,\n",
    "    \"copy_times\": [copy_times]*copy_times,\n",
    "\n",
    "    # Defining the total number of neurons of the model.\n",
    "    \"N\": [17000]*copy_times,\n",
    "\n",
    "    # Defining the bounds used in the random topology model.\n",
    "    \"bounds\": [[0.6, 0.6, 0.6]]*copy_times,\n",
    "    \n",
    "    # Defining the number of neurons per mm^3 and the excitatory ratio for each of the layers.\n",
    "    \"layer_names\": [['L1', 'L2_L3', 'L4', 'L5', 'L6']]*copy_times,\n",
    "    \"layer_volumes\": [dict(layer_volumes)]*copy_times,\n",
    "    \"layer_densities\": [dict(layer_densities_dict)]*copy_times,\n",
    "    \"excitatory_ratios\": [dict(layer_excitatory_ratios_dict)]*copy_times,\n",
    "\n",
    "    # Defining the potassium equilibrium potential for both the excitatory and inhibitory neurons.\n",
    "    # - Healthy mode: Eke_baseline = -90mV\n",
    "    # - Epileptic mode: Eke_baseline = -84mV\n",
    "    \"Eke_baseline\": [-84*mV]*copy_times, \n",
    "    \"Eki_baseline\": [-90*mV]*copy_times,\n",
    "\n",
    "    # Defining the noise affecting the excitatory and inhibitory neurons.\n",
    "    \"noise_exc\": [[0.07, 0.075]*nA]*copy_times, # OLD: [0.1045, 0.104]\n",
    "    \"noise_inh\": [[0.05, 0.08]*nA]*copy_times,\n",
    "\n",
    "    # Defining the base probabilities of connections between neurons from different layers:\n",
    "    # - p_e2e => Probability of an excitatory to excitatory neuron (synapse) connection.\n",
    "    # - p_e2i => Probability of an excitatory to inhibitory neuron (synapse) connection.\n",
    "    # - p_i2e => Probability of an inhibitory to excitatory neuron (synapse) connection.\n",
    "    # - p_i2i => Probability of an inhibitory to inhibitory neuron (synapse) connection.\n",
    "    # Normal ranges from 0.7-0.75, to activate sprouting increase the normal by 0.5\n",
    "    # This will increase the average number of excitatory connections by 500.\n",
    "    \"p\": [[0.75, 0.35, 0.35, 0.0]]*copy_times, \n",
    "\n",
    "    # Defining from which file the stimulus originates.\n",
    "    \"input_signal_file\": ['sigmoid-1.0.txt']*copy_times, \n",
    "\n",
    "    ## Defining which parameters can be tweaked for the stimulus mask:\n",
    "    # - stimulus layers: The layers that will feature the stimulus mask.\n",
    "    # - stimulus center coordinates: The center coordinates of the stimulus mask.\n",
    "    # - stimulus radius/edge length: The radius/edge length of the stimulus mask.\n",
    "    # - stimulus shape: The shape of the stimulus mask.\n",
    "    \"stimulus_layers\": [['L4']]*copy_times,\n",
    "    \"topology_names_stimulus\": [['L4_exc']]*copy_times,\n",
    "    \"stimulus_center_coordinates\": [[0.16, 0.8475, 0.16]]*copy_times,\n",
    "    \"stimulus_radius_or_edge_length\": [0.06]*copy_times,\n",
    "    \"shape_stimulus_mask\": ['all']*copy_times, # 'spherical', 'cubical', 'all', 'perc_of_all'\n",
    "    \"stimulus_mask_all_perc\": [50]*copy_times, # used for the 'create_prec_of_all_mask()' function.\n",
    "    \n",
    "    ## Defining which parameters are needed for the creation of the treatment mask:\n",
    "    # - treatment layers: The layers that will feature the treatment mask.\n",
    "    # - stimulus center coordinates: The center coordinates of the stimulus mask.\n",
    "    # - stimulus radius/edge length: The radius/edge length of the stimulus mask.\n",
    "    # - treatment shape: The shape of the treatment mask.\n",
    "    \"treatment_layers\": [['L2_L3']]*copy_times,\n",
    "    \"topology_names_treatment\": [['L2_L3_exc', 'L2_L3_inh']]*copy_times,\n",
    "    \"treatment_center_coordinates\": [[0.16, 1.06, 0.16]]*copy_times,\n",
    "    \"treatment_radius_or_edge_length\": [0.14]*copy_times,\n",
    "    \"shape_treatment_mask\": ['spherical']*copy_times, # 'spherical', 'cubical', 'all', 'perc_of_all'\n",
    "    \"treatment_mask_all_perc\": [50]*copy_times, # used for the 'create_prec_of_all_mask()' function.\n",
    "    \"distance_between_masks\": [0]*copy_times, # the distance required between the stimulus and treatment mask.\n",
    "\n",
    "    ## Defining other parameters that can be tweaked for treatment:\n",
    "    # - firing rate threshold: Rate at which the treatment should be activated.\n",
    "    # - device sensitivity: Frequency with which is checked if the firing rate is above the threshold.\n",
    "    # - Eke treatment: The potassium equilibrium potential for the excitatory neurons once treatment is activated.\n",
    "    # - Eke treatment: The potassium equilibrium potential for the inhibitory neurons once treatment is activated (unchanged from untreated).\n",
    "    \"firing_rate_threshold\": [5*Hz]*copy_times,\n",
    "    \"device_sensitivity\": [8*ms]*copy_times,\n",
    "    \"Eke_treatment\": [-100*mV]*copy_times,\n",
    "    \"Eki_treatment\": [-90*mV]*copy_times,\n",
    "\n",
    "    # Defining which layer should feature the LFP electrode.\n",
    "    \"LFP_electrode_layer\": ['L4']*copy_times\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83411d6b-3854-4032-b1eb-e387906af638",
   "metadata": {},
   "source": [
    "<br></br><br></br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8643c399-9a7c-4e4e-8a53-afee09595e84",
   "metadata": {},
   "source": [
    "<h2 style=\"font-size: 40px;\">Running the Simulation</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6431c23d-c5cc-49f9-981d-92b7005d135b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Running the simulation with the variables.\n",
    "run_model_loop(variables)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
